import hashlib
from telegram import InlineKeyboardButton, InlineKeyboardMarkup
from telegram.ext import ContextTypes
import re
from typing import List
import difflib

# -----------------------------
# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
# -----------------------------
BOOKS_PER_PAGE = 10

ARABIC_STOP_WORDS = {
    "Ùˆ", "ÙÙŠ", "Ù…Ù†", "Ø¥Ù„Ù‰", "Ø¹Ù†", "Ø¹Ù„Ù‰", "Ø¨", "Ù„", "Ø§", "Ø£Ùˆ", "Ø£Ù†", "Ø¥Ø°Ø§",
    "Ù…Ø§", "Ù‡Ø°Ø§", "Ù‡Ø°Ù‡", "Ø°Ù„Ùƒ", "ØªÙ„Ùƒ", "ÙƒØ§Ù†", "Ù‚Ø¯", "Ø§Ù„Ø°ÙŠ", "Ø§Ù„ØªÙŠ", "Ù‡Ùˆ", "Ù‡ÙŠ",
    "Ù", "Ùƒ", "Ø§Ù‰"
}

# -----------------------------
# Ø¯ÙˆØ§Ù„ Ø§Ù„ØªØ·Ø¨ÙŠØ¹ ÙˆØ§Ù„ØªÙ†Ø¸ÙŠÙ
# -----------------------------
def normalize_text(text: str) -> str:
    if not text:
        return ""
    text = text.lower()
    text = text.replace("_", " ")
    text = text.replace("Ø£", "Ø§").replace("Ø¥", "Ø§").replace("Ø¢", "Ø§")
    text = text.replace("Ù‰", "ÙŠ").replace("Ø©", "Ù‡")
    text = text.replace("Ù€", "")
    text = re.sub(r"[Ù‹ÙŒÙÙÙÙ]", "", text)
    text = re.sub(r'[^\w\s]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def light_stem(word: str) -> str:
    suffixes = ["ÙŠØ©", "ÙŠ", "ÙˆÙ†", "Ø§Øª", "Ø§Ù†", "ÙŠÙ†", "Ù‡"]
    for suf in suffixes:
        if word.endswith(suf) and len(word) > len(suf) + 2:
            word = word[:-len(suf)]
            break
    if word.startswith("Ø§Ù„") and len(word) > 3:
        word = word[2:]
    return word if word else ""

def expand_keywords_with_synonyms(keywords: List[str], synonyms: dict) -> List[str]:
    expanded = set(keywords)
    for k in keywords:
        if k in synonyms:
            expanded.update(synonyms[k])
    return list(expanded)

# -----------------------------
# Ø¥Ø±Ø³Ø§Ù„ Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª Ø§Ù„Ø¨Ø­Ø«
# -----------------------------
async def send_search_suggestions(update, context: ContextTypes.DEFAULT_TYPE, query: str, books_list: List[dict], synonyms: dict):
    """
    ØªØ¹Ø±Ø¶ Ø§Ù„Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª Ø¹Ù†Ø¯Ù…Ø§ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†ØªØ§Ø¦Ø¬ Ù…Ø¨Ø§Ø´Ø±Ø© Ù„Ù„Ø¨Ø­Ø«
    """
    normalized_query = normalize_text(query)
    query_words = [w for w in normalized_query.split() if w not in ARABIC_STOP_WORDS]
    stemmed_query = [light_stem(w) for w in query_words]
    
    # Ø¬Ù…Ø¹ ÙƒÙ„ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„ÙƒØªØ¨ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ù…Ø³Ø¨Ù‚Ø§Ù‹
    all_titles = [b["file_name"] for b in books_list]
    
    # Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª Ø¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰ ØªØ´Ø§Ø¨Ù‡ Ù†ØµÙŠ Ø¨Ø³ÙŠØ·
    suggestions = difflib.get_close_matches(normalized_query, all_titles, n=5, cutoff=0.5)
    
    # Ù„Ùˆ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª Ø¯Ù‚ÙŠÙ‚Ø©ØŒ Ù†Ø¨Ø­Ø« ÙÙŠ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© ÙˆØ§Ù„Ù…Ø±Ø§Ø¯ÙØ§Øª
    if not suggestions:
        expanded = expand_keywords_with_synonyms(stemmed_query, synonyms)
        for b in books_list:
            title_norm = normalize_text(b["file_name"])
            if any(word in title_norm for word in expanded):
                suggestions.append(b["file_name"])
    
    # Ø¥Ø°Ø§ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø·Ù„Ø§Ù‚
    if not suggestions:
        await update.message.reply_text(f"âŒ Ù„Ø§ ØªÙˆØ¬Ø¯ ÙƒØªØ¨ Ù…Ø·Ø§Ø¨Ù‚Ø© Ù„Ù„Ø¨Ø­Ø«: {query}\nØ­Ø§ÙˆÙ„ ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø£Ùˆ ØªØ¬Ø±Ø¨Ø© ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© Ø£Ø®Ø±Ù‰.")
        return
    
    # ØªØ¬Ù‡ÙŠØ² Ù„ÙˆØ­Ø© Ø§Ù„Ù…ÙØ§ØªÙŠØ­ Ù„Ù„Ø£Ø²Ø±Ø§Ø±
    keyboard = []
    for title in suggestions:
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† file_id Ù„Ù„ÙƒØªØ§Ø¨
        file_id = None
        for b in books_list:
            if b["file_name"] == title:
                file_id = b.get("file_id")
                break
        if file_id:
            key = hashlib.md5(file_id.encode()).hexdigest()[:16]
            context.bot_data[f"file_{key}"] = file_id
            keyboard.append([InlineKeyboardButton(title, callback_data=f"file:{key}")])
    
    reply_markup = InlineKeyboardMarkup(keyboard)
    await update.message.reply_text(f"ğŸ” Ù„Ù… Ù†Ø¹Ø«Ø± Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬ Ù…Ø·Ø§Ø¨Ù‚Ø© Ù„Ù„Ø¨Ø­Ø« '{query}'ØŒ Ù„ÙƒÙ† Ù‡Ø°Ù‡ Ø¨Ø¹Ø¶ Ø§Ù„Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª:", reply_markup=reply_markup)
