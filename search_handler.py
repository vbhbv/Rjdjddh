import hashlib
from telegram import InlineKeyboardButton, InlineKeyboardMarkup
from telegram.ext import ContextTypes
import re
from typing import List, Dict, Any
import os
import asyncio
import pyarabic.araby as araby
from unidecode import unidecode

# -----------------------------
# Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ÙˆÙ‚Ø§Ø¦Ù…Ø© Stop Words
# -----------------------------
BOOKS_PER_PAGE = 10

ARABIC_STOP_WORDS = {
    "Ùˆ", "ÙÙŠ", "Ù…Ù†", "Ø¥Ù„Ù‰", "Ø¹Ù†", "Ø¹Ù„Ù‰", "Ø¨", "Ù„", "Ø§", "Ø£Ùˆ", "Ø£Ù†", "Ø¥Ø°Ø§",
    "Ù…Ø§", "Ù‡Ø°Ø§", "Ù‡Ø°Ù‡", "Ø°Ù„Ùƒ", "ØªÙ„Ùƒ", "ÙƒØ§Ù†", "Ù‚Ø¯", "Ø§Ù„Ø°ÙŠ", "Ø§Ù„ØªÙŠ", "Ù‡Ùˆ", "Ù‡ÙŠ",
    "Ù", "Ùƒ", "Ø§Ù‰"
}

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ø´Ø±Ù
try:
    ADMIN_USER_ID = int(os.getenv("ADMIN_ID", "0"))
except ValueError:
    ADMIN_USER_ID = 0
    print("âš ï¸ ADMIN_ID environment variable is not valid.")

# -----------------------------
# Ø¯ÙˆØ§Ù„ Ø§Ù„ØªØ·Ø¨ÙŠØ¹ ÙˆØ§Ù„ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
# -----------------------------
def normalize_text(text: str) -> str:
    if not text: return ""
    text = str(text).lower()
    text = unidecode(text)
    text = araby.strip_tashkeel(text)
    text = araby.normalize_hamza(text)
    text = text.replace("Ù€", "")
    text = re.sub(r'[^\w\s]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def remove_common_words(text: str) -> str:
    if not text: return ""
    for word in ["ÙƒØªØ§Ø¨", "Ø±ÙˆØ§ÙŠØ©", "Ù†Ø³Ø®Ø©", "Ù…Ø¬Ù…ÙˆØ¹Ø©", "Ù…Ø¬Ù„Ø¯", "Ø¬Ø²Ø¡", "Ø·Ø¨Ø¹Ø©", "Ù…Ø¬Ø§Ù†ÙŠ", "ÙƒØ¨ÙŠØ±", "ØµØºÙŠØ±"]:
        text = text.replace(word, "")
    return text.strip()

def extract_keywords(text: str) -> List[str]:
    if not text: return []
    clean_text = normalize_text(text)
    words = clean_text.split()
    keywords = [w for w in words if w not in ARABIC_STOP_WORDS and len(w) >= 1]
    stop_words_for_search = [w for w in words if w in ARABIC_STOP_WORDS]
    return list(set(keywords + stop_words_for_search))

def light_stem(word: str) -> str:
    suffixes = ["ÙŠØ©", "ÙŠ", "ÙˆÙ†", "Ø§Øª", "Ø§Ù†", "ÙŠÙ†", "Ù‡"]
    for suf in suffixes:
        if word.endswith(suf) and len(word) > len(suf) + 2:
            word = word[:-len(suf)]
            break
    if word.startswith("Ø§Ù„") and len(word) > 3:
        word = word[2:]
    return word if word else ""

# -----------------------------
# Ø§Ù„Ù…Ø±Ø§Ø¯ÙØ§Øª Ù„ØªØ­Ø³ÙŠÙ† Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø­Ø«
# -----------------------------
SYNONYMS = {
    "Ù…Ù‡Ù†Ø¯Ø³": ["Ù‡Ù†Ø¯Ø³Ø©", "Ù…Ù‚Ø§ÙˆÙ„", "Ù…Ø¹Ù…Ø§Ø±ÙŠ"],
    "Ø§Ù„Ù‡Ù†Ø¯Ø³Ø©": ["Ù…Ù‡Ù†Ø¯Ø³", "Ù…Ø¹Ù…Ø§Ø±", "Ø¨Ù†Ø§Ø¡"],
    "Ø§Ù„Ù…Ù‡Ø¯ÙŠ": ["Ø§Ù„Ù…Ù†Ù‚Ø°", "Ø§Ù„Ù‚Ø§Ø¦Ù…"],
    "Ø¹Ø¯Ù…ÙŠØ©": ["Ù†ÙŠØªØ´Ù‡", "Ù…ÙˆØª", "Ø¹Ø¨Ø«"],
    "Ø¯ÙŠÙ†": ["Ø¥Ø³Ù„Ø§Ù…", "Ù…Ø³ÙŠØ­ÙŠØ©", "ÙŠÙ‡ÙˆØ¯ÙŠØ©", "ÙÙ‚Ù‡"],
    "ÙÙ„Ø³ÙØ©": ["Ù…Ù†Ø·Ù‚", "Ù…ÙÙ‡ÙˆÙ…", "Ù…ØªØ§ÙÙŠØ²ÙŠÙ‚Ø§"],
    "ØµÙˆÙÙŠØ©": ["ØªØµÙˆÙ", "Ø·Ø±Ù‚ ØµÙˆÙÙŠØ©", "Ø§Ù„Ø£ÙˆÙ„ÙŠØ§Ø¡", "Ø±ÙˆØ­Ø§Ù†ÙŠØ©"]
}

def expand_keywords_with_synonyms(keywords: List[str]) -> List[str]:
    expanded = set(keywords)
    for k in keywords:
        if k in SYNONYMS:
            expanded.update(SYNONYMS[k])
    return list(expanded)

# -----------------------------
# ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¨Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù‚ØµÙŠØ±Ø© ÙˆØ§Ù„Ø·ÙˆÙŠÙ„Ø©
# -----------------------------
def calculate_score(book: Dict[str, Any], query_keywords: List[str], normalized_query: str) -> int:
    score = 0
    book_name = normalize_text(book.get('file_name', ''))

    if normalized_query == book_name:
        score += 200
    elif normalized_query in book_name:
        score += 100

    title_words = book_name.split()
    for k in query_keywords:
        k_len = len(k)
        is_significant_short = k_len <= 3 and k not in ARABIC_STOP_WORDS
        base_match_score = 30 if k_len > 3 else 20
        base_stem_score = 15 if k_len > 3 else 10
        k_stem = light_stem(k)
        if not k_stem: continue

        for t_word in title_words:
            t_stem = light_stem(t_word)
            if t_stem.startswith(k_stem) and len(k_stem) >= 2:
                score += base_match_score * 2
            elif k_stem in t_stem:
                score += base_stem_score
            elif k in t_word:
                score += 5
            if is_significant_short and k == t_word:
                score += 50
    return score

# -----------------------------
# Ø¥Ø´Ø¹Ø§Ø± Ø§Ù„Ù…Ø´Ø±Ù
# -----------------------------
async def notify_admin_search(context: ContextTypes.DEFAULT_TYPE, username: str, query: str, found: bool):
    if ADMIN_USER_ID == 0: return
    bot = context.bot
    status_text = "âœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬" if found else "âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬"
    username_text = f"@{username}" if username else "(Ø¨Ø¯ÙˆÙ† ÙŠÙˆØ²Ø±)"
    message = f"ğŸ”” Ù‚Ø§Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… {username_text} Ø¨Ø§Ù„Ø¨Ø­Ø« Ø¹Ù†:\n`{query}`\nØ§Ù„Ø­Ø§Ù„Ø©: {status_text}"
    try:
        await bot.send_message(ADMIN_USER_ID, message, parse_mode='Markdown')
    except Exception as e:
        print(f"Failed to notify admin: {e}")

# -----------------------------
# Ø¹Ø±Ø¶ ØµÙØ­Ø© Ø§Ù„ÙƒØªØ¨
# -----------------------------
async def send_books_page(update, context: ContextTypes.DEFAULT_TYPE, include_index_home: bool = False):
    books = context.user_data.get("search_results", [])
    page = context.user_data.get("current_page", 0)
    search_stage = context.user_data.get("search_stage", "ØªØ·Ø§Ø¨Ù‚ Ø¯Ù‚ÙŠÙ‚")
    total_pages = (len(books) - 1) // BOOKS_PER_PAGE + 1 if books else 1

    start = page * BOOKS_PER_PAGE
    end = start + BOOKS_PER_PAGE
    current_books = books[start:end]

    if "Ø¨Ø­Ø« Ù…ÙˆØ³Ø¹" in search_stage or "Ø§Ù„Ø¬Ø°ÙˆØ±" in search_stage:
        stage_note = "âš ï¸ Ù†ØªØ§Ø¦Ø¬ Ø¨Ø­Ø« Ù…ÙˆØ³Ø¹ (Ø¨Ø­Ø«Ù†Ø§ Ø¨Ø§Ù„Ø¬Ø°ÙˆØ± ÙˆØ§Ù„Ù…Ø±Ø§Ø¯ÙØ§Øª)"
    elif "ØªØ·Ø§Ø¨Ù‚ Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª" in search_stage:
        stage_note = "âœ… Ù†ØªØ§Ø¦Ø¬ Ø¯Ù„Ø§Ù„ÙŠØ© (ØªØ·Ø§Ø¨Ù‚ Ø¬Ù…ÙŠØ¹ ÙƒÙ„Ù…Ø§ØªÙƒ Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©)"
    else:
        stage_note = "âœ… Ù†ØªØ§Ø¦Ø¬ Ù…Ø·Ø§Ø¨Ù‚Ø© (ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ø¹Ø¨Ø§Ø±Ø© ÙƒØ§Ù…Ù„Ø©)"

    text = f"ğŸ“š Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ({len(books)} ÙƒØªØ§Ø¨)\n{stage_note}\nØ§Ù„ØµÙØ­Ø© {page + 1} Ù…Ù† {total_pages}\n\n"
    keyboard = []

    for b in current_books:
        if not b.get("file_name") or not b.get("file_id"):
            continue
        key = hashlib.md5(b["file_id"].encode()).hexdigest()[:16]
        context.bot_data[f"file_{key}"] = b["file_id"]
        keyboard.append([InlineKeyboardButton(f"ğŸ“˜ {b['file_name']}", callback_data=f"file:{key}")])

    nav_buttons = []
    if page > 0:
        nav_buttons.append(InlineKeyboardButton("â¬…ï¸ Ø§Ù„Ø³Ø§Ø¨Ù‚", callback_data="prev_page"))
    if end < len(books):
        nav_buttons.append(InlineKeyboardButton("Ø§Ù„ØªØ§Ù„ÙŠ â¡ï¸", callback_data="next_page"))
    if nav_buttons:
        keyboard.append(nav_buttons)

    if context.user_data.get("is_index", False) or include_index_home:
        keyboard.append([InlineKeyboardButton("ğŸ  Ø§Ù„Ø¹ÙˆØ¯Ø© Ù„Ù„ÙÙ‡Ø±Ø³", callback_data="home_index")])

    reply_markup = InlineKeyboardMarkup(keyboard)
    if update.message:
        await update.message.reply_text(text, reply_markup=reply_markup)
    elif update.callback_query:
        await update.callback_query.message.edit_text(text, reply_markup=reply_markup)

# -----------------------------
# Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø°ÙƒÙŠ PostgreSQL + FTS + pg_trgm
# -----------------------------
async def search_books(update, context: ContextTypes.DEFAULT_TYPE):
    if update.effective_chat.type != "private": return
    query = update.message.text.strip()
    if not query: return

    conn = context.bot_data.get("db_conn")
    if not conn:
        await update.message.reply_text("âŒ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± Ù…ØªØµÙ„Ø© Ø­Ø§Ù„ÙŠØ§Ù‹.")
        return

    # ØªØ«Ø¨ÙŠØª Ø§Ù„Ø§Ù…ØªØ¯Ø§Ø¯ pg_trgm ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§
    try:
        await conn.execute("CREATE EXTENSION IF NOT EXISTS pg_trgm;")
    except:
        pass

    normalized_query = normalize_text(remove_common_words(query))
    all_words_in_query = normalize_text(query).split()
    keywords = [w for w in all_words_in_query if w not in ARABIC_STOP_WORDS and len(w) >= 1]
    expanded_keywords = expand_keywords_with_synonyms(keywords)
    stemmed_keywords = [light_stem(k) for k in expanded_keywords]

    context.user_data["last_query"] = normalized_query
    context.user_data["last_keywords"] = keywords

    search_stage_text = "Ø¨Ø­Ø« Ø¯Ù‚ÙŠÙ‚ FTS + Trigram"
    books = []

    try:
        # ØªØ­Ø¯ÙŠØ« Ø¹Ù…ÙˆØ¯ tsv_content Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±
        await conn.execute("""
            UPDATE books SET tsv_content = to_tsvector('simple', file_name)
            WHERE tsv_content IS NULL OR uploaded_at > (NOW() - INTERVAL '1 day')
        """)

        # Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ø¯Ù…Ø¬
        books = await conn.fetch("""
            SELECT id, file_id, file_name, uploaded_at,
            (ts_rank(tsv_content, websearch_to_tsquery('simple', $1)) * 0.7
            + similarity(file_name, $1) * 0.3) AS final_score
            FROM books
            WHERE tsv_content @@ websearch_to_tsquery('simple', $1)
            OR similarity(file_name, $1) > 0.3
            ORDER BY final_score DESC, uploaded_at DESC
            LIMIT 100
        """, normalized_query)

    except Exception as e:
        await update.message.reply_text(f"âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø«: {e}")
        return

    found_results = bool(books)
    await notify_admin_search(context, update.effective_user.username, query, found_results)

    if not books:
        keyboard = InlineKeyboardMarkup([[InlineKeyboardButton("ğŸ” Ø¨Ø­Ø« Ø¹Ù† ÙƒØªØ¨ Ù…Ø´Ø§Ø¨Ù‡Ø©", callback_data="search_similar")]])
        await update.message.reply_text(f"âŒ Ù„Ù… Ø£Ø¬Ø¯ Ø£ÙŠ ÙƒØªØ¨ Ù…Ø·Ø§Ø¨Ù‚Ø© Ù„Ù„Ø¨Ø­Ø«: {query}\nÙŠÙ…ÙƒÙ†Ùƒ ØªØ¬Ø±Ø¨Ø© Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙƒØªØ¨ Ù…Ø´Ø§Ø¨Ù‡Ø©:", reply_markup=keyboard)
        context.user_data["search_results"] = []
        context.user_data["current_page"] = 0
        return

    scored_books = []
    for book in books:
        score = calculate_score(book, all_words_in_query, normalized_query)
        book_dict = dict(book)
        book_dict['score'] = score
        scored_books.append(book_dict)

    scored_books.sort(key=lambda b: (b['score'], b['uploaded_at']), reverse=True)
    context.user_data["search_results"] = scored_books
    context.user_data["current_page"] = 0
    context.user_data["search_stage"] = search_stage_text
    await send_books_page(update, context)

# -----------------------------
# Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙƒØªØ¨ Ù…Ø´Ø§Ø¨Ù‡Ø©
# -----------------------------
async def search_similar_books(update, context: ContextTypes.DEFAULT_TYPE):
    conn = context.bot_data.get("db_conn")
    keywords = context.user_data.get("last_keywords")
    last_query = context.user_data.get("last_query", "")

    if not keywords or not conn:
        await update.callback_query.message.reply_text("âŒ Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù…ÙˆØ¶ÙˆØ¹ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù†Ù‡.")
        return

    try:
        expanded_keywords = expand_keywords_with_synonyms(keywords)
        stemmed_keywords = [light_stem(k) for k in expanded_keywords]
        search_terms = list(set(expanded_keywords + stemmed_keywords))

        books = await conn.fetch("""
            SELECT id, file_id, file_name, uploaded_at,
            (ts_rank(tsv_content, websearch_to_tsquery('simple', $1)) * 0.7
            + similarity(file_name, $1) * 0.3) AS final_score
            FROM books
            WHERE tsv_content @@ websearch_to_tsquery('simple', $1)
            OR similarity(file_name, $1) > 0.3
            ORDER BY final_score DESC, uploaded_at DESC
            LIMIT 100
        """, last_query)

    except Exception as e:
        await update.callback_query.message.edit_text(f"âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙƒØªØ¨ Ù…Ø´Ø§Ø¨Ù‡Ø©: {e}")
        return

    scored_books = []
    for book in books:
        all_words_in_query = normalize_text(last_query).split()
        score = calculate_score(book, all_words_in_query, last_query)
        book_dict = dict(book)
        book_dict['score'] = score
        scored_books.append(book_dict)

    scored_books.sort(key=lambda b: (b['score'], b['uploaded_at']), reverse=True)
    if not scored_books:
        await update.callback_query.message.edit_text("âŒ Ù„Ù… Ø£Ø¬Ø¯ ÙƒØªØ¨ Ù…Ø´Ø§Ø¨Ù‡Ø©.")
        return

    context.user_data["search_results"] = scored_books
    context.user_data["current_page"] = 0
    context.user_data["search_stage"] = "Ø¨Ø­Ø« Ù…ÙˆØ³Ø¹ (Ù…Ø´Ø§Ø¨Ù‡ Ø¨Ø§Ù„Ø¬Ø°ÙˆØ± ÙˆØ§Ù„Ù…Ø±Ø§Ø¯ÙØ§Øª)"
    await send_books_page(update, context)

# -----------------------------
# Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø£Ø²Ø±Ø§Ø± Ø§Ù„ÙƒØªØ¨ + Ø§Ù„ÙÙ‡Ø±Ø³
# -----------------------------
async def handle_callbacks(update, context: ContextTypes.DEFAULT_TYPE):
    query = update.callback_query
    await query.answer()
    data = query.data

    if data.startswith("file:"):
        key = data.split(":")[1]
        file_id = context.bot_data.get(f"file_{key}")
        if file_id:
            caption = "ØªÙ… Ø§Ù„ØªÙ†Ø²ÙŠÙ„ Ø¨ÙˆØ§Ø³Ø·Ø© @boooksfree1bot"
            share_button = InlineKeyboardMarkup([
                [InlineKeyboardButton("ğŸ“¤ Ø´Ø§Ø±Ùƒ Ø§Ù„Ø¨ÙˆØª Ù…Ø¹ Ø£ØµØ¯Ù‚Ø§Ø¦Ùƒ", switch_inline_query="")]
            ])
            await query.message.reply_document(document=file_id, caption=caption, reply_markup=share_button)
        else:
            await query.message.reply_text("âŒ Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…ØªÙˆÙØ± Ø­Ø§Ù„ÙŠØ§Ù‹.")

    elif data == "next_page":
        context.user_data["current_page"] += 1
        await send_books_page(update, context)
    elif data == "prev_page":
        context.user_data["current_page"] -= 1
        await send_books_page(update, context)
    elif data == "search_similar":
        await search_similar_books(update, context)

    elif data == "home_index" or data == "show_index":
        from index_handler import show_index
        await show_index(update, context)
    elif data.startswith("index_page:"):
        from index_handler import navigate_index_pages
        await navigate_index_pages(update, context)
    elif data.startswith("index:"):
        from index_handler import search_by_index
        await search_by_index(update, context)
