import hashlib
from telegram import InlineKeyboardButton, InlineKeyboardMarkup
from telegram.ext import ContextTypes
import re
from typing import List, Dict, Any
import os
import asyncio

# -----------------------------
# ÿßŸÑÿ•ÿπÿØÿßÿØÿßÿ™
# -----------------------------
BOOKS_PER_PAGE = 10

ARABIC_STOP_WORDS = {
    "Ÿà", "ŸÅŸä", "ŸÖŸÜ", "ÿ•ŸÑŸâ", "ÿπŸÜ", "ÿπŸÑŸâ", "ÿ®", "ŸÑ", "ÿß", "ÿ£Ÿà", "ÿ£ŸÜ", "ÿ•ÿ∞ÿß",
    "ŸÖÿß", "Ÿáÿ∞ÿß", "Ÿáÿ∞Ÿá", "ÿ∞ŸÑŸÉ", "ÿ™ŸÑŸÉ", "ŸÉÿßŸÜ", "ŸÇÿØ", "ÿßŸÑÿ∞Ÿä", "ÿßŸÑÿ™Ÿä", "ŸáŸà", "ŸáŸä",
    "ŸÅ", "ŸÉ", "ÿßŸâ"
}

# ÿ•ÿπÿØÿßÿØÿßÿ™ ÿßŸÑŸÖÿ¥ÿ±ŸÅ
try:
    ADMIN_USER_ID = int(os.getenv("ADMIN_ID", "0"))
except ValueError:
    ADMIN_USER_ID = 0
    print("‚ö†Ô∏è ADMIN_ID environment variable is not valid.")

# -----------------------------
# ÿØŸàÿßŸÑ ÿßŸÑÿ™ÿ∑ÿ®Ÿäÿπ ŸàÿßŸÑÿ™ŸÜÿ∏ŸäŸÅ
# -----------------------------
def normalize_text(text: str) -> str:
    if not text:
        return ""
    text = str(text).lower()
    text = text.replace("_", " ")
    text = text.replace("ÿ£", "ÿß").replace("ÿ•", "ÿß").replace("ÿ¢", "ÿß")
    text = text.replace("Ÿâ", "Ÿä").replace("ÿ©", "Ÿá")
    text = text.replace("ŸÄ", "")
    text = re.sub(r"[ŸãŸåŸçŸéŸèŸê]", "", text)
    text = re.sub(r'[^\w\s]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def remove_common_words(text: str) -> str:
    for word in ["ŸÉÿ™ÿßÿ®", "ÿ±ŸàÿßŸäÿ©", "ŸÜÿ≥ÿÆÿ©", "ŸÖÿ¨ŸÖŸàÿπÿ©", "ŸÖÿ¨ŸÑÿØ", "ÿ¨ÿ≤ÿ°", "ÿ∑ÿ®ÿπÿ©", "ŸÖÿ¨ÿßŸÜŸä", "ŸÉÿ®Ÿäÿ±", "ÿµÿ∫Ÿäÿ±"]:
        text = text.replace(word, "")
    return text.strip()

def light_stem(word: str) -> str:
    suffixes = ["Ÿäÿ©", "Ÿä", "ŸàŸÜ", "ÿßÿ™", "ÿßŸÜ", "ŸäŸÜ", "Ÿá"]
    for suf in suffixes:
        if word.endswith(suf) and len(word) > len(suf) + 2:
            word = word[:-len(suf)]
            break
    if word.startswith("ÿßŸÑ") and len(word) > 3:
        word = word[2:]
    return word

# -----------------------------
# ÿßŸÑŸÖÿ±ÿßÿØŸÅÿßÿ™
# -----------------------------
SYNONYMS = {
    "ŸÖŸáŸÜÿØÿ≥": ["ŸáŸÜÿØÿ≥ÿ©", "ŸÖŸÇÿßŸàŸÑ", "ŸÖÿπŸÖÿßÿ±Ÿä"],
    "ÿßŸÑŸáŸÜÿØÿ≥ÿ©": ["ŸÖŸáŸÜÿØÿ≥", "ŸÖÿπŸÖÿßÿ±", "ÿ®ŸÜÿßÿ°"],
    "ÿßŸÑŸÖŸáÿØŸä": ["ÿßŸÑŸÖŸÜŸÇÿ∞", "ÿßŸÑŸÇÿßÿ¶ŸÖ"],
    "ÿπÿØŸÖŸäÿ©": ["ŸÜŸäÿ™ÿ¥Ÿá", "ŸÖŸàÿ™", "ÿπÿ®ÿ´"],
    "ÿØŸäŸÜ": ["ÿ•ÿ≥ŸÑÿßŸÖ", "ŸÖÿ≥Ÿäÿ≠Ÿäÿ©", "ŸäŸáŸàÿØŸäÿ©", "ŸÅŸÇŸá"],
    "ŸÅŸÑÿ≥ŸÅÿ©": ["ŸÖŸÜÿ∑ŸÇ", "ŸÖŸÅŸáŸàŸÖ", "ŸÖÿ™ÿßŸÅŸäÿ≤ŸäŸÇÿß"],
    "ÿµŸàŸÅŸäÿ©": ["ÿ™ÿµŸàŸÅ", "ÿ∑ÿ±ŸÇ ÿµŸàŸÅŸäÿ©", "ÿßŸÑÿ£ŸàŸÑŸäÿßÿ°", "ÿ±Ÿàÿ≠ÿßŸÜŸäÿ©"]
}

def expand_keywords_with_synonyms(keywords: List[str]) -> List[str]:
    expanded = set(keywords)
    for k in keywords:
        if k in SYNONYMS:
            expanded.update(SYNONYMS[k])
    return list(expanded)

# -----------------------------
# ÿ•ÿ¥ÿπÿßÿ± ÿßŸÑŸÖÿ¥ÿ±ŸÅ
# -----------------------------
async def notify_admin_search(context: ContextTypes.DEFAULT_TYPE, username: str, query: str, found: bool):
    if ADMIN_USER_ID == 0:
        return
    bot = context.bot
    status_text = "‚úÖ ÿ™ŸÖ ÿßŸÑÿπÿ´Ÿàÿ± ÿπŸÑŸâ ŸÜÿ™ÿßÿ¶ÿ¨" if found else "‚ùå ŸÑŸÖ Ÿäÿ™ŸÖ ÿßŸÑÿπÿ´Ÿàÿ± ÿπŸÑŸâ ŸÜÿ™ÿßÿ¶ÿ¨"
    username_text = f"@{username}" if username else "(ÿ®ÿØŸàŸÜ ŸäŸàÿ≤ÿ±)"
    message = f"üîî ŸÇÿßŸÖ ÿßŸÑŸÖÿ≥ÿ™ÿÆÿØŸÖ {username_text} ÿ®ÿßŸÑÿ®ÿ≠ÿ´ ÿπŸÜ:\n`{query}`\nÿßŸÑÿ≠ÿßŸÑÿ©: {status_text}"
    try:
        await bot.send_message(ADMIN_USER_ID, message, parse_mode='Markdown')
    except Exception as e:
        print(f"Failed to notify admin: {e}")

# -----------------------------
# ÿπÿ±ÿ∂ ÿµŸÅÿ≠ÿ© ÿßŸÑŸÉÿ™ÿ®
# -----------------------------
async def send_books_page(update, context: ContextTypes.DEFAULT_TYPE, include_index_home: bool = False):
    books = context.user_data.get("search_results", [])
    page = context.user_data.get("current_page", 0)
    search_stage = context.user_data.get("search_stage", "ÿ™ÿ∑ÿßÿ®ŸÇ ÿØŸÇŸäŸÇ")
    total_pages = (context.user_data.get("total_books", 0) - 1) // BOOKS_PER_PAGE + 1

    start = page * BOOKS_PER_PAGE
    end = start + BOOKS_PER_PAGE
    current_books = books[start:end]

    if "ÿ®ÿ≠ÿ´ ŸÖŸàÿ≥ÿπ" in search_stage or "ÿßŸÑÿ¨ÿ∞Ÿàÿ±" in search_stage:
        stage_note = "‚ö†Ô∏è ŸÜÿ™ÿßÿ¶ÿ¨ ÿ®ÿ≠ÿ´ ŸÖŸàÿ≥ÿπ (ÿßŸÑÿ¨ÿ∞Ÿàÿ± ŸàÿßŸÑŸÖÿ±ÿßÿØŸÅÿßÿ™)"
    else:
        stage_note = "‚úÖ ŸÜÿ™ÿßÿ¶ÿ¨ ÿØŸÇŸäŸÇÿ©"

    text = f"üìö ÿßŸÑŸÜÿ™ÿßÿ¶ÿ¨ ({context.user_data.get('total_books', 0)} ŸÉÿ™ÿßÿ®)\n{stage_note}\nÿßŸÑÿµŸÅÿ≠ÿ© {page + 1} ŸÖŸÜ {total_pages}\n\n"
    keyboard = []

    for b in current_books:
        if not b.get("file_name") or not b.get("file_id"):
            continue
        key = hashlib.md5(b["file_id"].encode()).hexdigest()[:16]
        context.bot_data[f"file_{key}"] = b["file_id"]
        # ÿ•ÿ≤ÿßŸÑÿ© ÿßŸÑŸÉÿ™ÿßÿ® ÿßŸÑÿ£ÿ≤ÿ±ŸÇ Ÿàÿßÿ≥ÿ™ÿ®ÿØÿßŸÑŸá ÿ®ÿπŸÑÿßŸÖÿ© üîπ
        keyboard.append([InlineKeyboardButton(f"üîπ {b['file_name']}", callback_data=f"file:{key}")])

    nav_buttons = []
    if page > 0:
        nav_buttons.append(InlineKeyboardButton("‚¨ÖÔ∏è ÿßŸÑÿ≥ÿßÿ®ŸÇ", callback_data="prev_page"))
    if end < context.user_data.get("total_books", 0):
        nav_buttons.append(InlineKeyboardButton("ÿßŸÑÿ™ÿßŸÑŸä ‚û°Ô∏è", callback_data="next_page"))
    if nav_buttons:
        keyboard.append(nav_buttons)

    if context.user_data.get("is_index", False) or include_index_home:
        keyboard.append([InlineKeyboardButton("üè† ÿßŸÑÿπŸàÿØÿ© ŸÑŸÑŸÅŸáÿ±ÿ≥", callback_data="home_index")])

    reply_markup = InlineKeyboardMarkup(keyboard)
    if update.message:
        await update.message.reply_text(text, reply_markup=reply_markup)
    elif update.callback_query:
        await update.callback_query.message.edit_text(text, reply_markup=reply_markup)

# -----------------------------
# ÿßŸÑÿ®ÿ≠ÿ´ ÿßŸÑÿ∞ŸÉŸä ŸÖÿπ Pagination
# -----------------------------
async def search_books(update, context: ContextTypes.DEFAULT_TYPE):
    if update.effective_chat.type != "private":
        return
    query = update.message.text.strip()
    if not query:
        return

    conn = context.bot_data.get("db_conn")
    if not conn:
        await update.message.reply_text("‚ùå ŸÇÿßÿπÿØÿ© ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ÿ∫Ÿäÿ± ŸÖÿ™ÿµŸÑÿ© ÿ≠ÿßŸÑŸäÿßŸã.")
        return

    try:
        await conn.execute("CREATE EXTENSION IF NOT EXISTS pg_trgm;")
    except:
        pass

    normalized_query = normalize_text(remove_common_words(query))
    all_words_in_query = normalize_text(query).split()
    keywords = [w for w in all_words_in_query if w not in ARABIC_STOP_WORDS and len(w) >= 1]
    expanded_keywords = expand_keywords_with_synonyms(keywords)

    # ÿ®ŸÜÿßÿ° ÿßÿ≥ÿ™ÿπŸÑÿßŸÖ FTS ŸÖÿπ ÿßŸÑŸÖÿ±ÿßÿØŸÅÿßÿ™
    tsquery = ' & '.join([f"{k}:*" for k in expanded_keywords])
    context.user_data["last_query"] = normalized_query
    context.user_data["last_keywords"] = keywords
    context.user_data["current_page"] = 0

    # ÿ¨ŸÑÿ® ÿ£ŸàŸÑ ÿµŸÅÿ≠ÿ©
    page = 0
    offset = page * BOOKS_PER_PAGE

    try:
        total_books = await conn.fetchval("""
            SELECT COUNT(*) FROM books
            WHERE tsv_content @@ to_tsquery('arabic', $1)
        """, tsquery)

        rows = await conn.fetch("""
            SELECT id, file_id, file_name, uploaded_at,
            (ts_rank(tsv_content, to_tsquery('arabic', $1)) * 0.7
            + similarity(file_name, $2) * 0.3) AS final_score
            FROM books
            WHERE tsv_content @@ to_tsquery('arabic', $1)
            OR similarity(file_name, $2) > 0.3
            ORDER BY final_score DESC, uploaded_at DESC
            LIMIT $3 OFFSET $4
        """, tsquery, normalized_query, BOOKS_PER_PAGE, offset)

    except Exception as e:
        await update.message.reply_text(f"‚ùå ÿ≠ÿØÿ´ ÿÆÿ∑ÿ£ ŸÅŸä ÿßŸÑÿ®ÿ≠ÿ´: {e}")
        return

    found_results = bool(rows)
    await notify_admin_search(context, update.effective_user.username, query, found_results)

    if not rows:
        await update.message.reply_text(f"‚ùå ŸÑŸÖ ÿ£ÿ¨ÿØ ÿ£Ÿä ŸÉÿ™ÿ® ŸÖÿ∑ÿßÿ®ŸÇÿ© ŸÑŸÑÿ®ÿ≠ÿ´: {query}")
        context.user_data["search_results"] = []
        context.user_data["total_books"] = 0
        return

    context.user_data["search_results"] = [dict(row) for row in rows]
    context.user_data["total_books"] = total_books
    context.user_data["search_stage"] = "ÿ®ÿ≠ÿ´ ÿØŸÇŸäŸÇ FTS + Trigram"
    await send_books_page(update, context)

# -----------------------------
# ÿßŸÑÿ™ÿπÿßŸÖŸÑ ŸÖÿπ ÿ£ÿ≤ÿ±ÿßÿ± ÿßŸÑŸÉÿ™ÿ® + ÿßŸÑŸÅŸáÿ±ÿ≥
# -----------------------------
async def handle_callbacks(update, context: ContextTypes.DEFAULT_TYPE):
    query = update.callback_query
    await query.answer()
    data = query.data

    if data.startswith("file:"):
        key = data.split(":")[1]
        file_id = context.bot_data.get(f"file_{key}")
        if file_id:
            caption = "ÿ™ŸÖ ÿßŸÑÿ™ŸÜÿ≤ŸäŸÑ ÿ®Ÿàÿßÿ≥ÿ∑ÿ© @boooksfree1bot"
            share_button = InlineKeyboardMarkup([
                [InlineKeyboardButton("üì§ ÿ¥ÿßÿ±ŸÉ ÿßŸÑÿ®Ÿàÿ™ ŸÖÿπ ÿ£ÿµÿØŸÇÿßÿ¶ŸÉ", switch_inline_query="")]
            ])
            await query.message.reply_document(document=file_id, caption=caption, reply_markup=share_button)
        else:
            await query.message.reply_text("‚ùå ÿßŸÑŸÖŸÑŸÅ ÿ∫Ÿäÿ± ŸÖÿ™ŸàŸÅÿ± ÿ≠ÿßŸÑŸäÿßŸã.")
    elif data == "next_page":
        context.user_data["current_page"] += 1
        await search_books_page(update, context)
    elif data == "prev_page":
        context.user_data["current_page"] -= 1
        await search_books_page(update, context)
    elif data == "home_index" or data == "show_index":
        from index_handler import show_index
        await show_index(update, context)

# -----------------------------
# ÿ¨ŸÑÿ® ÿßŸÑÿµŸÅÿ≠ÿ© ÿßŸÑÿ™ÿßŸÑŸäÿ© ŸÖÿ®ÿßÿ¥ÿ±ÿ© ŸÖŸÜ DB
# -----------------------------
async def search_books_page(update, context):
    query = context.user_data.get("last_query", "")
    if not query:
        return
    page = context.user_data.get("current_page", 0)
    offset = page * BOOKS_PER_PAGE
    conn = context.bot_data.get("db_conn")
    if not conn:
        return

    all_words_in_query = context.user_data.get("last_keywords", [])
    tsquery = ' & '.join([f"{k}:*" for k in all_words_in_query])

    rows = await conn.fetch("""
        SELECT id, file_id, file_name, uploaded_at,
        (ts_rank(tsv_content, to_tsquery('arabic', $1)) * 0.7
        + similarity(file_name, $2) * 0.3) AS final_score
        FROM books
        WHERE tsv_content @@ to_tsquery('arabic', $1)
        OR similarity(file_name, $2) > 0.3
        ORDER BY final_score DESC, uploaded_at DESC
        LIMIT $3 OFFSET $4
    """, tsquery, query, BOOKS_PER_PAGE, offset)

    context.user_data["search_results"] = [dict(row) for row in rows]
    await send_books_page(update, context)
