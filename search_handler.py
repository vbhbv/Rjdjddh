import hashlib
import re
import logging
import os
from typing import List
from telegram import InlineKeyboardButton, InlineKeyboardMarkup
from telegram.ext import ContextTypes

# Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

BOOKS_PER_PAGE = 10
ADMIN_USER_ID = int(os.getenv("ADMIN_ID", "0"))

class BookSearchEngine:
    """Ù…Ø­Ø±Ùƒ Ø¨Ø­Ø« Ù‡Ø¬ÙŠÙ† ÙŠØ¯Ù…Ø¬ Ø¨ÙŠÙ† Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ù„ÙØ¸ÙŠ ÙˆØ§Ù„Ø¨Ø­Ø« Ø§Ù„Ù†ØµÙŠ Ø§Ù„ÙƒØ§Ù…Ù„"""
    
    @staticmethod
    def normalize_for_db(text: str) -> str:
        """ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ù†Øµ Ù„ÙŠØªÙˆØ§ÙÙ‚ Ù…Ø¹ Ø£Ø³Ù„ÙˆØ¨ ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        if not text: return ""
        text = text.lower().strip()
        # ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ø­Ø±ÙˆÙ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ØµØ¹Ø¨Ø©
        replacements = str.maketrans("Ø£Ø¥Ø¢Ø©Ù‰", "Ø§Ø§Ø§ÙˆÙ‡")
        text = text.translate(replacements)
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø±Ù…ÙˆØ²
        text = re.sub(r'[^\w\s]', ' ', text)
        return ' '.join(text.split())

    @classmethod
    async def perform_search(cls, conn, query: str):
        normalized_q = cls.normalize_for_db(query)
        keywords = [f"{w}:*" for w in normalized_q.split() if len(w) > 1]
        fts_query = " & ".join(keywords) if keywords else normalized_q

        # Ø§Ø³ØªØ¹Ù„Ø§Ù… SQL ÙˆØ§Ø­Ø¯ ÙŠØ¬Ù…Ø¹ ÙƒÙ„ Ø§Ù„Ù…Ø±Ø§Ø­Ù„ ÙˆÙŠØ±ØªØ¨Ù‡Ø§ Ø­Ø³Ø¨ Ø§Ù„Ø£Ù‡Ù…ÙŠØ© (Weighting)
        # 1. Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„ØªØ§Ù…Ø© ØªØ£Ø®Ø° Ø§Ù„ÙˆØ²Ù† Ø§Ù„Ø£Ø¹Ù„Ù‰
        # 2. Ø§Ù„ØªØ´Ø§Ø¨Ù‡ (Trigram) ÙŠØ¹Ø§Ù„Ø¬ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ø¥Ù…Ù„Ø§Ø¦ÙŠØ©
        # 3. Ø§Ù„ØªØ±ØªÙŠØ¨ Ø§Ù„Ù†ØµÙŠ (Rank) ÙŠØ¹Ø§Ù„Ø¬ Ø¯Ù‚Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª
        sql = """
        SELECT id, file_id, file_name,
               (CASE WHEN file_name ILIKE $1 THEN 1.0 ELSE 0 END) as exact_score,
               similarity(file_name, $2) as sim_score,
               ts_rank_cd(to_tsvector('arabic', file_name), to_tsquery('arabic', $3)) as fts_score
        FROM books
        WHERE 
            file_name % $2  -- Ø§Ø³ØªØ®Ø¯Ø§Ù… index trgm
            OR to_tsvector('arabic', file_name) @@ to_tsquery('arabic', $3)
            OR file_name ILIKE $4
        ORDER BY 
            exact_score DESC, 
            (sim_score * 0.6 + fts_score * 0.4) DESC
        LIMIT 150;
        """
        like_query = f"%{normalized_q}%"
        exact_query = f"{normalized_q}"
        
        return await conn.fetch(sql, exact_query, normalized_q, fts_query, like_query)

# ==========================
# Ù…Ø¹Ø§Ù„Ø¬Ø© ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„ØªÙ„ÙŠØ¬Ø±Ø§Ù…
# ==========================

async def search_books(update, context: ContextTypes.DEFAULT_TYPE):
    if not update.message or not update.message.text: return
    
    query = update.message.text.strip()
    conn = context.bot_data.get("db_conn")
    
    if not conn:
        await update.message.reply_text("âŒ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± Ù…ØªØµÙ„Ø©.")
        return

    # Ø¥Ø¸Ù‡Ø§Ø± Ø­Ø§Ù„Ø© "Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¨Ø­Ø«" Ù„ØªØ­Ø³ÙŠÙ† ØªØ¬Ø±Ø¨Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
    status_msg = await update.message.reply_text("ğŸ” Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù…ÙƒØªØ¨Ø©...")

    try:
        results = await BookSearchEngine.perform_search(conn, query)
        
        if not results:
            from search_suggestions import send_search_suggestions
            await status_msg.delete()
            await send_search_suggestions(update, context)
            return

        # ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        context.user_data["search_results"] = [dict(b) for b in results]
        context.user_data["current_page"] = 0
        context.user_data["search_stage"] = "âœ… Ù†ØªØ§Ø¦Ø¬ Ø°ÙƒÙŠØ© Ù…ØªÙ‚Ø¯Ù…Ø©"

        await status_msg.delete()
        await send_books_page(update, context)

    except Exception as e:
        logger.error(f"Search Error: {e}")
        await status_msg.edit_text("âš ï¸ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø·Ù„Ø¨Ùƒ.")

async def send_books_page(update, context: ContextTypes.DEFAULT_TYPE):
    # (Ù†ÙØ³ Ø¯Ø§Ù„Ø© send_books_page Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© Ù…Ø¹ ØªØ­Ø³ÙŠÙ† Ø¨Ø³ÙŠØ· ÙÙŠ Ø¹Ø±Ø¶ Ø§Ù„Ø£Ø³Ù…Ø§Ø¡)
    data = context.user_data
    books = data.get("search_results", [])
    page = data.get("current_page", 0)
    
    start = page * BOOKS_PER_PAGE
    end = start + BOOKS_PER_PAGE
    current_batch = books[start:end]
    total_pages = (len(books) + BOOKS_PER_PAGE - 1) // BOOKS_PER_PAGE

    text = f"ğŸ“š Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ({len(books)} ÙƒØªØ§Ø¨)\n{data.get('search_stage')}\nØ§Ù„ØµÙØ­Ø© {page + 1} Ù…Ù† {total_pages}"
    
    keyboard = []
    for b in current_batch:
        key = hashlib.md5(str(b["file_id"]).encode()).hexdigest()[:12]
        context.bot_data[f"f_{key}"] = b["file_id"]
        keyboard.append([InlineKeyboardButton(f"ğŸ“– {b['file_name'][:60]}", callback_data=f"file:{key}")])

    # Ø£Ø²Ø±Ø§Ø± Ø§Ù„ØªÙ†Ù‚Ù„
    nav = []
    if page > 0: nav.append(InlineKeyboardButton("â¬…ï¸ Ø§Ù„Ø³Ø§Ø¨Ù‚", callback_data="prev_page"))
    if end < len(books): nav.append(InlineKeyboardButton("Ø§Ù„ØªØ§Ù„ÙŠ â¡ï¸", callback_data="next_page"))
    if nav: keyboard.append(nav)
    
    reply_markup = InlineKeyboardMarkup(keyboard)
    
    if update.callback_query:
        await update.callback_query.message.edit_text(text, reply_markup=reply_markup)
    else:
        await update.message.reply_text(text, reply_markup=reply_markup)
