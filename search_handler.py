import hashlib
import re
import logging
from typing import List
from telegram import InlineKeyboardButton, InlineKeyboardMarkup
from telegram.ext import ContextTypes

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù„ÙˆØ¬ Ù„ØªØªØ¨Ø¹ Ø£ÙŠ Ø£Ø®Ø·Ø§Ø¡
logger = logging.getLogger(__name__)

# Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
BOOKS_PER_PAGE = 10
MAX_RESULTS = 500  # Ø¹Ø¯Ø¯ ÙƒØ§ÙÙ Ø¬Ø¯Ø§Ù‹ ÙˆØ´Ø§Ù…Ù„ ÙˆØ¯Ù‚ÙŠÙ‚

# Ø¯Ø§Ù„Ø© Ø§Ù„ØªØ·Ø¨ÙŠØ¹ (ÙŠØ¬Ø¨ Ø£Ù† ØªØªØ·Ø§Ø¨Ù‚ Ù…Ø¹ Ù…Ù†Ø·Ù‚ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª)
def normalize_query(text: str) -> str:
    if not text: return ""
    text = text.lower().strip()
    repls = str.maketrans("Ø£Ø¥Ø¢Ø©Ù‰", "Ø§Ø§Ø§ÙˆÙ‡")
    text = text.translate(repls)
    text = re.sub(r"[Ù‹ÙŒÙÙÙÙÙ’Ù‘Ù€]", "", text)
    text = re.sub(r'[^\w\s]', ' ', text)
    return ' '.join(text.split())

# ØªÙ†Ø¸ÙŠÙ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠØ©
def get_clean_keywords(text: str) -> List[str]:
    stop_words = {"Ø±ÙˆØ§ÙŠØ©", "ØªØ­Ù…ÙŠÙ„", "ÙƒØªØ§Ø¨", "Ù…Ø¬Ø§Ù†ÙŠ", "pdf", "Ù†Ø³Ø®Ø©"}
    words = text.split()
    if len(words) <= 2: 
        return words
    return [w for w in words if w not in stop_words]

async def search_books(update, context: ContextTypes.DEFAULT_TYPE):
    query = update.message.text.strip()
    conn = context.bot_data.get("db_conn")

    if not conn:
        await update.message.reply_text("âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.")
        return

    norm_q = normalize_query(query)
    keywords = get_clean_keywords(norm_q)
    ts_query = ' & '.join([f"{w}:*" for w in keywords])

    try:
        sql = """
        SELECT file_id, file_name,
               ts_rank_cd(to_tsvector('arabic', file_name), to_tsquery('arabic', $1)) AS rank,
               similarity(file_name, $2) AS sim
        FROM books
        WHERE 
            to_tsvector('arabic', file_name) @@ to_tsquery('arabic', $1)
            OR file_name ILIKE $3
            OR file_name % $2
        ORDER BY 
            (file_name ILIKE $3) DESC,
            rank DESC, 
            sim DESC
        LIMIT $4;
        """
        
        full_pattern = f"%{query.strip()}%"
        rows = await conn.fetch(sql, ts_query, norm_q, full_pattern, MAX_RESULTS)

        if not rows:
            from search_suggestions import send_search_suggestions
            context.user_data["last_query"] = query
            await send_search_suggestions(update, context)
            return

        context.user_data["search_results"] = [dict(r) for r in rows]
        context.user_data["current_page"] = 0
        context.user_data["search_stage"] = "âœ… Ù†ØªØ§Ø¦Ø¬ Ø°ÙƒÙŠØ©"
        await send_books_page(update, context)

    except Exception as e:
        logger.error(f"Search Error: {e}")
        await update.message.reply_text("âš ï¸ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø¨Ø­Ø«ØŒ ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù„Ø§Ø­Ù‚Ø§Ù‹.")

# Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
async def send_books_page(update, context: ContextTypes.DEFAULT_TYPE):
    results = context.user_data.get("search_results", [])
    page = context.user_data.get("current_page", 0)
    
    start = page * BOOKS_PER_PAGE
    end = start + BOOKS_PER_PAGE
    current_batch = results[start:end]
    total_pages = (len(results) - 1) // BOOKS_PER_PAGE + 1

    text = f"ğŸ“š **Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø­Ø« ({len(results)} Ù†ØªÙŠØ¬Ø©):**\n"
    text += f"ØµÙØ­Ø© {page + 1} Ù…Ù† {total_pages}\n\n"
    
    keyboard = []
    for b in current_batch:
        clean_name = b['file_name'] if len(b['file_name']) < 50 else b['file_name'][:47] + "..."
        key = hashlib.md5(b['file_id'].encode()).hexdigest()[:16]
        context.bot_data[f"file_{key}"] = b['file_id']
        keyboard.append([InlineKeyboardButton(f"ğŸ“– {clean_name}", callback_data=f"file:{key}")])

    nav_buttons = []
    if page > 0:
        nav_buttons.append(InlineKeyboardButton("â¬…ï¸ Ø§Ù„Ø³Ø§Ø¨Ù‚", callback_data="prev_page"))
    if end < len(results):
        nav_buttons.append(InlineKeyboardButton("Ø§Ù„ØªØ§Ù„ÙŠ â¡ï¸", callback_data="next_page"))
    
    if nav_buttons:
        keyboard.append(nav_buttons)

    reply_markup = InlineKeyboardMarkup(keyboard)
    
    if update.message:
        await update.message.reply_text(text, reply_markup=reply_markup, parse_mode="Markdown")
    else:
        await update.callback_query.message.edit_text(text, reply_markup=reply_markup, parse_mode="Markdown")

# =========================
# Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ù‡Ù†Ø§ ÙÙ‚Ø·
# =========================
async def handle_callbacks(update, context: ContextTypes.DEFAULT_TYPE):
    query = update.callback_query
    data = query.data
    await query.answer()

    if data.startswith("file:"):
        key = data.split(":")[1]
        file_id = context.bot_data.get(f"file_{key}")
        if file_id:
            await query.message.reply_document(
                document=file_id,
                caption="ØªÙ… ØªÙ†Ø²ÙŠÙ„ Ø§Ù„ÙƒØªØ§Ø¨ Ø¨ÙˆØ§Ø³Ø·Ø© @boooksfree1bot",
                reply_markup=InlineKeyboardMarkup([
                    [InlineKeyboardButton("ğŸ“¤ Ù…Ø´Ø§Ø±ÙƒØ© Ø§Ù„ÙƒØªØ§Ø¨", switch_inline_query="")]
                ])
            )
        else:
            await query.message.reply_text("âŒ Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø§Ù†ØªÙ‡Øª ØµÙ„Ø§Ø­ÙŠØ© Ù‡Ø°Ø§ Ø§Ù„Ø±Ø§Ø¨Ø·. Ø§Ø¨Ø­Ø« Ù…Ø¬Ø¯Ø¯Ø§Ù‹.")
            
    elif data == "next_page":
        context.user_data["current_page"] += 1
        await send_books_page(update, context)
    elif data == "prev_page":
        context.user_data["current_page"] -= 1
        await send_books_page(update, context)
