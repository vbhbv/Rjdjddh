import hashlib
from telegram import InlineKeyboardButton, InlineKeyboardMarkup
from telegram.ext import ContextTypes
import re
from typing import List
import os

# -----------------------------
# Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ÙˆÙ‚Ø§Ø¦Ù…Ø© Stop Words
# -----------------------------
BOOKS_PER_PAGE = 10

ARABIC_STOP_WORDS = {
    "Ùˆ", "ÙÙŠ", "Ù…Ù†", "Ø¥Ù„Ù‰", "Ø¹Ù†", "Ø¹Ù„Ù‰", "Ø¨", "Ù„", "Ø§", "Ø£Ùˆ", "Ø£Ù†", "Ø¥Ø°Ø§",
    "Ù…Ø§", "Ù‡Ø°Ø§", "Ù‡Ø°Ù‡", "Ø°Ù„Ùƒ", "ØªÙ„Ùƒ", "ÙƒØ§Ù†", "Ù‚Ø¯", "Ø§Ù„Ø°ÙŠ", "Ø§Ù„ØªÙŠ", "Ù‡Ùˆ", "Ù‡ÙŠ",
    "Ù", "Ùƒ", "Ø§Ù‰"
}

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ø´Ø±Ù
try:
    ADMIN_USER_ID = int(os.getenv("ADMIN_ID", "0"))
except ValueError:
    ADMIN_USER_ID = 0
    print("âš ï¸ ADMIN_ID environment variable is not valid.")

# -----------------------------
# Ø¯ÙˆØ§Ù„ Ø§Ù„ØªØ·Ø¨ÙŠØ¹ ÙˆØ§Ù„ØªÙ†Ø¸ÙŠÙ
# -----------------------------
def normalize_text(text: str) -> str:
    if not text:
        return ""
    text = str(text).lower()
    text = text.replace("_", " ")
    text = text.replace("Ø£", "Ø§").replace("Ø¥", "Ø§").replace("Ø¢", "Ø§")
    text = text.replace("Ù‰", "ÙŠ").replace("Ø©", "Ù‡")
    text = text.replace("Ù€", "")
    text = re.sub(r"[Ù‹ÙŒÙÙÙÙ]", "", text)
    text = re.sub(r'[^\w\s]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def remove_common_words(text: str) -> str:
    if not text:
        return ""
    for word in ["ÙƒØªØ§Ø¨", "Ø±ÙˆØ§ÙŠØ©", "Ø§Ø±ÙŠØ¯", "Ù…Ø¬Ù…ÙˆØ¹Ø©", "Ù…Ø¬Ù„Ø¯", "Ø¬Ø²Ø¡", "Ø·Ø¨Ø¹Ø©", "Ù…Ø¬Ø§Ù†ÙŠ", "ÙƒØ¨ÙŠØ±", "ØµØºÙŠØ±"]:
        text = text.replace(word, "")
    return text.strip()

def light_stem(word: str) -> str:
    suffixes = ["ÙŠØ©", "Ù‡", "ÙŠ", "ÙˆÙ†", "Ø§Øª", "Ø§Ù†", "ÙŠÙ†", "Ù…Ù‡Ø¯ÙŠ", "Ø§Ù„Ù‡Ù†Ø¯"]
    for suf in suffixes:
        if word.endswith(suf) and len(word) > len(suf) + 1:
            word = word[:-len(suf)]
            break
    if word.startswith("Ø§Ù„") and len(word) > 3:
        word = word[2:]
    return word if word else ""

# -----------------------------
# Ø§Ù„Ù…Ø±Ø§Ø¯ÙØ§Øª
# -----------------------------
SYNONYMS = {
    "Ù…Ù‡Ø¯ÙŠ": ["Ø§Ù„Ù…Ù‡Ø¯ÙŠ", "Ø§Ù„Ù…Ù†Ù‚Ø°", "Ø§Ù„Ù‚Ø§Ø¦Ù…", "Ù…Ø­Ù…Ø¯ Ø§Ù„Ù…Ù‡Ø¯ÙŠ"],
    "Ø§Ù„Ù‡Ù†Ø¯Ø³Ø©": ["Ù…Ù‡Ù†Ø¯Ø³", "Ù‡Ù†Ø¯Ø³ÙŠ", "Ù‡Ù†Ø¯Ø³Ù‡", "Ù‡Ù†Ø¯Ø³Ø© Ù…Ø¹Ù…Ø§Ø±ÙŠØ©", "Ù…Ø¹Ù…Ø§Ø±"],
    "Ø¯ÙŠÙ†": ["ÙÙ‚Ù‡", "Ø§Ø³Ù„Ø§Ù…", "Ø´Ø±ÙŠØ¹Ø©"],
    "ÙÙ„Ø³ÙØ©": ["ØªÙÙƒÙŠØ±", "Ù…Ù†Ø·Ù‚", "Ù…ÙŠØªØ§ÙÙŠØ²ÙŠÙ‚Ø§"],
}

def expand_keywords_with_synonyms(keywords: List[str]) -> List[str]:
    expanded = set(keywords)
    for k in keywords:
        if k in SYNONYMS:
            expanded.update(SYNONYMS[k])
    return list(expanded)

# -----------------------------
# Ø¥Ø±Ø³Ø§Ù„ ØµÙØ­Ø© Ø§Ù„ÙƒØªØ¨
# -----------------------------
async def send_books_page(update, context: ContextTypes.DEFAULT_TYPE, include_index_home: bool = False):
    books = context.user_data.get("search_results", [])
    page = context.user_data.get("current_page", 0)
    search_stage = context.user_data.get("search_stage", "ØªØ·Ø§Ø¨Ù‚ Ø¯Ù‚ÙŠÙ‚")
    total_pages = (len(books) - 1) // BOOKS_PER_PAGE + 1 if books else 1

    start = page * BOOKS_PER_PAGE
    end = start + BOOKS_PER_PAGE
    current_books = books[start:end]

    if "Ø¨Ø­Ø« Ù…ÙˆØ³Ø¹" in search_stage:
        stage_note = "âš ï¸ Ù†ØªØ§Ø¦Ø¬ Ø¨Ø­Ø« Ù…ÙˆØ³Ø¹ (Ø¨Ø­Ø« Ø¬Ø°ÙˆØ± + Ù…Ø±Ø§Ø¯ÙØ§Øª + Trigram)"
    else:
        stage_note = "âœ… Ù†ØªØ§Ø¦Ø¬ Ø¯Ù‚ÙŠÙ‚Ø© Ù…Ø­Ø³Ù‘Ù†Ø©"

    text = f"ğŸ“š Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ({len(books)} ÙƒØªØ§Ø¨)\n{stage_note}\nØ§Ù„ØµÙØ­Ø© {page + 1} Ù…Ù† {total_pages}\n\n"
    keyboard = []

    for b in current_books:
        if not b.get("file_name") or not b.get("file_id"):
            continue
        key = hashlib.md5(b["file_id"].encode()).hexdigest()[:16]
        context.bot_data[f"file_{key}"] = b["file_id"]
        keyboard.append([InlineKeyboardButton(f"{b['file_name']}", callback_data=f"file:{key}")])

    nav_buttons = []
    if page > 0:
        nav_buttons.append(InlineKeyboardButton("â¬…ï¸ Ø§Ù„Ø³Ø§Ø¨Ù‚", callback_data="prev_page"))
    if end < len(books):
        nav_buttons.append(InlineKeyboardButton("Ø§Ù„ØªØ§Ù„ÙŠ â¡ï¸", callback_data="next_page"))
    if nav_buttons:
        keyboard.append(nav_buttons)

    if context.user_data.get("is_index", False) or include_index_home:
        keyboard.append([InlineKeyboardButton("ğŸ  Ø§Ù„Ø¹ÙˆØ¯Ø© Ù„Ù„ÙÙ‡Ø±Ø³", callback_data="home_index")])

    reply_markup = InlineKeyboardMarkup(keyboard)

    if update.message:
        await update.message.reply_text(text, reply_markup=reply_markup)
    elif update.callback_query:
        await update.callback_query.message.edit_text(text, reply_markup=reply_markup)

# -----------------------------
# Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø°ÙƒÙŠ Ø§Ù„Ù…Ø­Ø³Ù‘Ù†
# -----------------------------
async def search_books(update, context: ContextTypes.DEFAULT_TYPE):
    if update.effective_chat.type != "private":
        return

    query = update.message.text.strip()
    if not query:
        return

    conn = context.bot_data.get("db_conn")
    if not conn:
        await update.message.reply_text("âŒ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± Ù…ØªØµÙ„Ø© Ø­Ø§Ù„ÙŠØ§Ù‹.")
        return

    # -----------------------
    # Ù…Ø±Ø­Ù„Ø© ØªØ¬Ù‡ÙŠØ² Ø§Ù„ÙƒÙ„Ù…Ø§Øª
    # -----------------------
    normalized = normalize_text(remove_common_words(query))

    words = [w for w in normalized.split() if w not in ARABIC_STOP_WORDS]

    # Ù…Ø±Ø§Ø¯ÙØ§Øª + Ø¬Ø°Ø± + Ù…Ø·Ø§Ø¨Ù‚Ø© Ø¬Ø²Ø¦ÙŠØ©
    expanded = expand_keywords_with_synonyms(words)
    stems = [light_stem(w) for w in expanded]

    # Ø¨Ù†Ø§Ø¡ ØµÙŠØºØ© FTS Ù‚ÙˆÙŠØ©
    fts_all = " & ".join(stems)
    fts_any = " | ".join(expanded)

    # Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù‚ØµÙŠØ±Ø© Ù…Ø«Ù„ (Ø§Ù„Ù…Ù‡Ø¯ÙŠ) Ùˆ (Ø§Ù„Ù‡Ù†Ø¯Ø³Ø©)
    trigram_conditions = " OR ".join([f"file_name % '{w}'" for w in expanded])

    search_query = f"({fts_all}) | ({fts_any})"

    # -----------------------
    # ØªÙ†ÙÙŠØ° Ø§Ù„Ø¨Ø­Ø«
    # -----------------------
    try:
        books = await conn.fetch(f"""
            SELECT id, file_id, file_name, uploaded_at
            FROM books
            WHERE 
                to_tsvector('arabic', file_name) @@ to_tsquery('arabic', $1)
                OR {trigram_conditions}
            ORDER BY 
                similarity(file_name, $2) DESC,
                ts_rank(to_tsvector('arabic', file_name), to_tsquery('arabic', $1)) DESC
            LIMIT 200;
        """, search_query, normalized)
    except Exception as e:
        await update.message.reply_text(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø«: {e}")
        return

    if not books:
        await update.message.reply_text(f"âŒ Ù„Ù… Ø£Ø¬Ø¯ Ù†ØªØ§Ø¦Ø¬ Ù„Ù€: {query}")
        context.user_data["search_results"] = []
        return

    context.user_data["search_results"] = [dict(b) for b in books]
    context.user_data["current_page"] = 0
    context.user_data["search_stage"] = "Ø¨Ø­Ø« Ù…ÙˆØ³Ø¹ Ù…ØªÙ‚Ø¯Ù…"
    await send_books_page(update, context)

# -----------------------------
# Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø£Ø²Ø±Ø§Ø± Ø§Ù„ÙƒØªØ¨ + Ø§Ù„ÙÙ‡Ø±Ø³
# -----------------------------
async def handle_callbacks(update, context: ContextTypes.DEFAULT_TYPE):
    query = update.callback_query
    await query.answer()
    data = query.data

    if data.startswith("file:"):
        key = data.split(":")[1]
        file_id = context.bot_data.get(f"file_{key}")
        if file_id:
            caption = "ØªÙ… Ø§Ù„ØªÙ†Ø²ÙŠÙ„ Ø¨ÙˆØ§Ø³Ø·Ø© @boooksfree1bot"
            share_button = InlineKeyboardMarkup([[InlineKeyboardButton("Ø´Ø§Ø±Ùƒ Ø§Ù„Ø¨ÙˆØª", switch_inline_query="")]])
            await query.message.reply_document(document=file_id, caption=caption, reply_markup=share_button)
        else:
            await query.message.reply_text("âŒ Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯.")

    elif data == "next_page":
        context.user_data["current_page"] += 1
        await send_books_page(update, context)

    elif data == "prev_page":
        context.user_data["current_page"] -= 1
        await send_books_page(update, context)

    elif data in ("home_index", "show_index"):
        from index_handler import show_index
        await show_index(update, context)
