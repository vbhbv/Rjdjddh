import hashlib
import re
import logging
import os
from typing import List
from telegram import InlineKeyboardButton, InlineKeyboardMarkup
from telegram.ext import ContextTypes
from search_suggestions import send_search_suggestions

# =========================
# Logging
# =========================
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# =========================
# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø¹Ø§Ù…Ø©
# =========================
BOOKS_PER_PAGE = 10

# ÙƒÙ„Ù…Ø§Øª Ø§Ù„ØªÙˆÙ‚Ù: ØªÙ…Øª Ø¥Ø²Ø§Ù„Ø© "ÙƒØªØ¨" Ù…Ù†Ù‡Ø§ Ù„Ù„Ø³Ù…Ø§Ø­ Ø¨Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø³ÙŠØ§Ù‚ÙŠ
ARABIC_STOP_WORDS = {
    "Ùˆ", "ÙÙŠ", "Ù…Ù†", "Ø¥Ù„Ù‰", "Ø¹Ù†", "Ø¹Ù„Ù‰", "Ø¨", "Ù„", "Ø§", "Ø£Ùˆ", "Ø£Ù†", "Ø¥Ø°Ø§",
    "Ù…Ø§", "Ù‡Ø°Ø§", "Ù‡Ø°Ù‡", "Ø°Ù„Ùƒ", "ØªÙ„Ùƒ", "ÙƒØ§Ù†", "Ù‚Ø¯", "Ø§Ù„Ø°ÙŠ", "Ø§Ù„ØªÙŠ", "Ù‡Ùˆ", "Ù‡ÙŠ"
}

# =========================
# Ø¯ÙˆØ§Ù„ Ø§Ù„ØªØ·Ø¨ÙŠØ¹ ÙˆØ§Ù„ØªÙ†Ø¸ÙŠÙ
# =========================
def normalize_text(text: str) -> str:
    if not text: return ""
    text = str(text).lower().replace("_", " ")
    # ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ø­Ø±ÙˆÙ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ù…ØªØ´Ø§Ø¨Ù‡Ø© (Ø£ØŒ Ø¥ØŒ Ø¢ -> Ø§) Ùˆ (Ø© -> Ù‡) Ùˆ (Ù‰ -> ÙŠ)
    repls = str.maketrans("Ø£Ø¥Ø¢Ø©Ù‰", "Ø§Ø§Ø§ÙˆÙ‡")
    text = text.translate(repls)
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„ ÙˆØ§Ù„ØªØ·ÙˆÙŠÙ„ ÙˆØ§Ù„Ø±Ù…ÙˆØ²
    text = re.sub(r"[Ù‹ÙŒÙÙÙÙÙ’Ù‘Ù€]", "", text)
    text = re.sub(r'[^\w\s]', ' ', text)
    return ' '.join(text.split())

def clean_query_smart(text: str) -> List[str]:
    """ØªØ¬Ù‡ÙŠØ² Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ù…Ø¹ Ø§Ø³ØªØ¨Ø¹Ø§Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª ØºÙŠØ± Ø§Ù„Ù…Ø¤Ø«Ø±Ø© ÙÙ‚Ø·"""
    bad_words = {"Ø±ÙˆØ§ÙŠØ©", "Ù†Ø³Ø®Ø©", "Ù…Ø¬Ù…ÙˆØ¹Ø©", "Ø§Ø±ÙŠØ¯", "Ø¬Ø²Ø¡", "Ø·Ø¨Ø¹Ø©", "Ù…Ø¬Ø§Ù†ÙŠ", "ÙƒØ¨ÙŠØ±", "ØµØºÙŠØ±"}
    words = text.split()
    # Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ØªÙŠ Ø·ÙˆÙ„Ù‡Ø§ Ø£ÙƒØ¨Ø± Ù…Ù† Ø­Ø±ÙÙŠÙ† ÙˆÙ„ÙŠØ³Øª ÙÙŠ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…Ù†Ø¹
    return [w for w in words if w not in bad_words and w not in ARABIC_STOP_WORDS]

# =========================
# Ø¥Ø±Ø³Ø§Ù„ ØµÙØ­Ø© Ø§Ù„ÙƒØªØ¨ (UI)
# =========================
async def send_books_page(update, context: ContextTypes.DEFAULT_TYPE, include_index_home: bool = False):
    books = context.user_data.get("search_results", [])
    page = context.user_data.get("current_page", 0)
    search_stage = context.user_data.get("search_stage", "âœ… Ù†ØªØ§Ø¦Ø¬ Ù…Ø·Ø§Ø¨Ù‚Ø©")
    
    total_pages = (len(books) - 1) // BOOKS_PER_PAGE + 1 if books else 1
    start, end = page * BOOKS_PER_PAGE, (page + 1) * BOOKS_PER_PAGE
    current_books = books[start:end]

    text = f"ğŸ“š **Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ({len(books)} ÙƒØªØ§Ø¨)**\n{search_stage}\nØ§Ù„ØµÙØ­Ø© {page + 1} Ù…Ù† {total_pages}\n\n"
    keyboard = []

    for b in current_books:
        # ØªÙˆÙ„ÙŠØ¯ Ù…ÙØªØ§Ø­ ÙØ±ÙŠØ¯ Ù„Ù„Ù…Ù„Ù
        key = hashlib.md5(str(b["file_id"]).encode()).hexdigest()[:16]
        context.bot_data[f"file_{key}"] = b["file_id"]
        # ØªÙ‚ØµÙŠØ± Ø§Ù„Ø§Ø³Ù… Ø§Ù„Ø·ÙˆÙŠÙ„ Ø¬Ø¯Ø§Ù‹ Ù„ÙŠÙ†Ø§Ø³Ø¨ Ø£Ø²Ø±Ø§Ø± Ø§Ù„ØªÙ„Ø¬Ø±Ø§Ù…
        display_name = (b['file_name'][:57] + '..') if len(b['file_name']) > 60 else b['file_name']
        keyboard.append([InlineKeyboardButton(f"ğŸ“– {display_name}", callback_data=f"file:{key}")])

    nav = []
    if page > 0: nav.append(InlineKeyboardButton("â¬…ï¸ Ø§Ù„Ø³Ø§Ø¨Ù‚", callback_data="prev_page"))
    if end < len(books): nav.append(InlineKeyboardButton("Ø§Ù„ØªØ§Ù„ÙŠ â¡ï¸", callback_data="next_page"))
    if nav: keyboard.append(nav)

    if context.user_data.get("is_index", False) or include_index_home:
        keyboard.append([InlineKeyboardButton("ğŸ  Ø§Ù„Ø¹ÙˆØ¯Ø© Ù„Ù„ÙÙ‡Ø±Ø³", callback_data="home_index")])

    markup = InlineKeyboardMarkup(keyboard)
    if update.message:
        await update.message.reply_text(text, reply_markup=markup, parse_mode="Markdown")
    elif update.callback_query:
        await update.callback_query.message.edit_text(text, reply_markup=markup, parse_mode="Markdown")

# =========================
# Ù…Ø­Ø±Ùƒ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø°ÙƒÙŠ (Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©)
# =========================
async def search_books(update, context: ContextTypes.DEFAULT_TYPE):
    if update.effective_chat.type != "private": return
    query = update.message.text.strip()
    if not query or len(query) < 2: return

    conn = context.bot_data.get("db_conn")
    if not conn:
        await update.message.reply_text("âŒ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± Ù…ØªØµÙ„Ø©.")
        return

    # ØªÙ†Ø¸ÙŠÙ ÙˆØªØ¬Ù‡ÙŠØ² Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª
    norm_q = normalize_text(query)
    keywords = clean_query_smart(norm_q)
    
    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø¥Ù„Ù‰ ØµÙŠØºØ© Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù†ØµÙŠ Ø§Ù„ÙƒØ§Ù…Ù„ (FTS)
    # Ù…Ø«Ø§Ù„: "ÙƒØªØ¨ Ø¹Ø³ÙƒØ±ÙŠØ©" ØªØµØ¨Ø­ "ÙƒØªØ¨:* & Ø¹Ø³ÙƒØ±ÙŠØ©:*"
    ts_query = ' & '.join([f"{w}:*" for w in keywords]) if keywords else norm_q

    try:
        # Ø¶Ø¨Ø· Ø­Ø³Ø§Ø³ÙŠØ© Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ù„ÙØ¸ÙŠ (Ø§Ù„ØªÙˆØ§Ø²Ù† Ø¨ÙŠÙ† Ø§Ù„Ø¯Ù‚Ø© ÙˆØ§Ù„Ø³Ø±Ø¹Ø©)
        await conn.execute("SET pg_trgm.similarity_threshold = 0.3;")

        # Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ù‡Ø¬ÙŠÙ† Ø§Ù„Ù…ØªÙ‚Ø¯Ù…:
        # 1. ILIKE: Ù„Ù„Ø¨Ø­Ø« Ø§Ù„Ø¬Ø²Ø¦ÙŠ Ø§Ù„Ø¯Ù‚ÙŠÙ‚ (ÙŠØ­Ù„ Ù…Ø´ÙƒÙ„Ø© "Ø¹Ø³ÙƒØ±ÙŠØ©")
        # 2. FTS: Ù„Ù„Ø¨Ø­Ø« Ø¨Ø§Ù„Ù…Ø¹Ù†Ù‰ ÙˆØ§Ù„Ø¬Ø°ÙˆØ±
        # 3. Trigram: Ù„Ù„Ø¨Ø­Ø« Ø¨Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ù„ÙØ¸ÙŠ (Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ø¥Ù…Ù„Ø§Ø¦ÙŠØ©)
        sql = """
        SELECT id, file_id, file_name,
               ts_rank_cd(to_tsvector('arabic', file_name), to_tsquery('arabic', $1)) AS rank,
               similarity(file_name, $2) AS sim
        FROM books
        WHERE 
            to_tsvector('arabic', file_name) @@ to_tsquery('arabic', $1)
            OR file_name ILIKE $3
            OR file_name % $2
        ORDER BY 
            (file_name ILIKE $3) DESC, -- Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© Ø§Ù„Ù‚ØµÙˆÙ‰ Ù„ÙˆØ¬ÙˆØ¯ Ø§Ù„ÙƒÙ„Ù…Ø© Ø­Ø±ÙÙŠØ§Ù‹
            rank DESC, 
            sim DESC
        LIMIT 200;
        """
        
        # Ù†Ø£Ø®Ø° Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ø£Ø®ÙŠØ±Ø© Ø£Ùˆ Ø§Ù„Ø£Ù‡Ù… Ù„Ù„Ø¨Ø­Ø« Ø§Ù„Ø¬Ø²Ø¦ÙŠ (Ù…Ø«Ù„ "Ø¹Ø³ÙƒØ±ÙŠØ©")
        partial_pattern = f"%{keywords[-1]}%" if keywords else f"%{norm_q}%"
        
        rows = await conn.fetch(sql, ts_query, norm_q, partial_pattern)
        
        if not rows:
            # Ø¥Ø°Ø§ Ù„Ù… ÙŠØ¬Ø¯ Ø´ÙŠØ¦Ø§Ù‹ØŒ Ù†Ø³ØªØ®Ø¯Ù… Ù…Ø­Ø±Ùƒ Ø§Ù„Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª
            await send_search_suggestions(update, context)
            return

        context.user_data["search_results"] = [dict(r) for r in rows]
        context.user_data["current_page"] = 0
        context.user_data["search_stage"] = "âš¡ Ù†ØªØ§Ø¦Ø¬ Ø¨Ø­Ø« Ø°ÙƒÙŠ ÙØ§Ø¦Ù‚Ø© Ø§Ù„Ø³Ø±Ø¹Ø©"
        await send_books_page(update, context)

    except Exception as e:
        logger.error(f"Search error: {e}")
        # Ù…Ø­Ø§ÙˆÙ„Ø© Ø£Ø®ÙŠØ±Ø© Ø¨Ø¨Ø­Ø« Ø¨Ø³ÙŠØ· Ø¬Ø¯Ø§Ù‹ Ù„Ø¶Ù…Ø§Ù† Ø¹Ø¯Ù… Ø®ÙŠØ¨Ø© Ø£Ù…Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
        try:
            simple_rows = await conn.fetch("SELECT * FROM books WHERE file_name ILIKE $1 LIMIT 50", f"%{norm_q}%")
            if simple_rows:
                context.user_data["search_results"] = [dict(r) for r in simple_rows]
                await send_books_page(update, context)
            else:
                await update.message.reply_text("âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬ Ø¯Ù‚ÙŠÙ‚Ø©ØŒ Ø¬Ø±Ø¨ ÙƒÙ„Ù…Ø§Øª Ø£Ø®Ø±Ù‰.")
        except:
            await update.message.reply_text("âš ï¸ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙ†ÙŠØŒ ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù„Ø§Ø­Ù‚Ø§Ù‹.")

# ==========================
# Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø£Ø²Ø±Ø§Ø± Ø§Ù„ØªÙ†Ù‚Ù„ ÙˆØ§Ù„ØªØ­Ù…ÙŠÙ„
# ==========================
async def handle_callbacks(update, context: ContextTypes.DEFAULT_TYPE):
    query = update.callback_query
    await query.answer()
    data = query.data

    if data.startswith("file:"):
        key = data.split(":")[1]
        file_id = context.bot_data.get(f"file_{key}")
        if file_id:
            try:
                await query.message.reply_document(
                    document=file_id, 
                    caption="ğŸ“– ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒØªØ§Ø¨ Ù…Ù† Ø§Ù„Ù…ÙƒØªØ¨Ø© Ø§Ù„Ø´Ø§Ù…Ù„Ø©",
                    reply_markup=InlineKeyboardMarkup([[InlineKeyboardButton("ğŸ“¤ Ù…Ø´Ø§Ø±ÙƒØ© Ø§Ù„Ø¨ÙˆØª", switch_inline_query="")]])
                )
            except Exception as e:
                logger.error(f"Download error: {e}")
                await query.message.reply_text("âŒ Ø¹Ø°Ø±Ø§Ù‹ØŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù. Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ø§Ù„Ø±Ø§Ø¨Ø· Ù…Ù†ØªÙ‡ÙŠØ§Ù‹.")
        else:
            await query.message.reply_text("âŒ Ø§Ù„Ø±Ø§Ø¨Ø· Ù‚Ø¯ÙŠÙ…ØŒ ÙŠØ±Ø¬Ù‰ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙƒØªØ§Ø¨ Ù…Ø¬Ø¯Ø¯Ø§Ù‹.")
    
    elif data == "next_page":
        context.user_data["current_page"] = context.user_data.get("current_page", 0) + 1
        await send_books_page(update, context)
    elif data == "prev_page":
        context.user_data["current_page"] = max(0, context.user_data.get("current_page", 0) - 1)
        await send_books_page(update, context)
    elif data in ("home_index", "show_index"):
        try:
            from index_handler import show_index
            await show_index(update, context)
        except ImportError:
            await query.message.reply_text("ğŸ  Ø§Ù„Ø¹ÙˆØ¯Ø© Ù„Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©...")
