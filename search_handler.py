import hashlib
import re
from typing import List
import os
from telegram import InlineKeyboardButton, InlineKeyboardMarkup
from telegram.ext import ContextTypes
from search_suggestions import send_search_suggestions  # Ø§Ù„Ø±Ø¨Ø· Ø¨Ø§Ù„Ù…Ù„Ù Ø§Ù„Ø¬Ø¯ÙŠØ¯
import logging

=========================

Logging

=========================

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(name)

=========================

Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø¹Ø§Ù…Ø©

=========================

BOOKS_PER_PAGE = 10

ARABIC_STOP_WORDS = {
"Ùˆ", "ÙÙŠ", "Ù…Ù†", "Ø¥Ù„Ù‰", "Ø¹Ù†", "Ø¹Ù„Ù‰", "Ø¨", "Ù„", "Ø§", "Ø£Ùˆ", "Ø£Ù†", "Ø¥Ø°Ø§",
"Ù…Ø§", "Ù‡Ø°Ø§", "Ù‡Ø°Ù‡", "Ø°Ù„Ùƒ", "ØªÙ„Ùƒ", "ÙƒØ§Ù†", "Ù‚Ø¯", "Ø§Ù„Ø°ÙŠ", "Ø§Ù„ØªÙŠ", "Ù‡Ùˆ", "Ù‡ÙŠ",
"Ù", "Ùƒ", "Ø§Ù‰"
}

Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ø´Ø±Ù

try:
ADMIN_USER_ID = int(os.getenv("ADMIN_ID", "0"))
except ValueError:
ADMIN_USER_ID = 0
print("âš ï¸ ADMIN_ID environment variable is not valid.")

=========================

Ø¯ÙˆØ§Ù„ Ø§Ù„ØªØ·Ø¨ÙŠØ¹ ÙˆØ§Ù„ØªÙ†Ø¸ÙŠÙ

=========================

def normalize_text(text: str) -> str:
if not text:
return ""
text = str(text).lower()
text = text.replace("_", " ")
text = text.replace("Ø£", "Ø§").replace("Ø¥", "Ø§").replace("Ø¢", "Ø§")
text = text.replace("Ù‰", "ÙŠ").replace("Ø©", "Ù‡")
text = text.replace("Ù€", "")
text = re.sub(r"[Ù‹ÙŒÙÙÙÙ]", "", text)
text = re.sub(r'[^\w\s]', ' ', text)
text = re.sub(r'\s+', ' ', text).strip()
return text

def remove_common_words(text: str) -> str:
if not text:
return ""
for word in ["ÙƒØªØ§Ø¨", "Ø±ÙˆØ§ÙŠØ©", "Ù†Ø³Ø®Ø©", "Ù…Ø¬Ù…ÙˆØ¹Ø©", "Ø§Ø±ÙŠØ¯", "Ø¬Ø²Ø¡", "Ø·Ø¨Ø¹Ø©", "Ù…Ø¬Ø§Ù†ÙŠ", "ÙƒØ¨ÙŠØ±", "ØµØºÙŠØ±"]:
text = text.replace(word, "")
return text.strip()

def light_stem(word: str) -> str:
suffixes = ["ÙŠØ©", "ÙŠ", "ÙˆÙ†", "Ø§Øª", "Ø§Ù†", "ÙŠÙ†", "Ù‡"]
for suf in suffixes:
if word.endswith(suf) and len(word) > len(suf) + 2:
word = word[:-len(suf)]
break
if word.startswith("Ø§Ù„") and len(word) > 3:
word = word[2:]
return word if word else ""

=========================

Ø§Ù„Ù…Ø±Ø§Ø¯ÙØ§Øª

=========================

SYNONYMS = {
"Ù…Ù‡Ù†Ø¯Ø³": ["Ù‡Ù†Ø¯Ø³Ø©", "Ù…Ù‚Ø§ÙˆÙ„", "Ù…Ø¹Ù…Ø§Ø±ÙŠ"],
"Ø§Ù„Ù‡Ù†Ø¯Ø³Ø©": ["Ù…Ù‡Ù†Ø¯Ø³", "Ù…Ø¹Ù…Ø§Ø±", "Ø¨Ù†Ø§Ø¡"],
"Ø§Ù„Ù…Ù‡Ø¯ÙŠ": ["Ø§Ù„Ù…Ù†Ù‚Ø°", "Ø§Ù„Ù‚Ø§Ø¦Ù…"],
"Ø¹Ø¯Ù…ÙŠØ©": ["Ù†ÙŠØªØ´Ù‡", "Ù…ÙˆØª", "Ø¹Ø¨Ø«"],
"Ø¯ÙŠÙ†": ["Ø¥Ø³Ù„Ø§Ù…", "Ù…Ø³ÙŠØ­ÙŠØ©", "ÙŠÙ‡ÙˆØ¯ÙŠØ©", "ÙÙ‚Ù‡"],
"ÙÙ„Ø³ÙØ©": ["Ù…Ù†Ø·Ù‚", "Ù…ÙÙ‡ÙˆÙ…", "Ù…ØªØ§ÙÙŠØ²ÙŠÙ‚Ø§"]
}

def expand_keywords_with_synonyms(keywords: List[str]) -> List[str]:
expanded = set(keywords)
for k in keywords:
if k in SYNONYMS:
expanded.update(SYNONYMS[k])
return list(expanded)

=========================

Ø¥Ø±Ø³Ø§Ù„ ØµÙØ­Ø© Ø§Ù„ÙƒØªØ¨

=========================

async def send_books_page(update, context: ContextTypes.DEFAULT_TYPE, include_index_home: bool = False):
books = context.user_data.get("search_results", [])
page = context.user_data.get("current_page", 0)
search_stage = context.user_data.get("search_stage", "ØªØ·Ø§Ø¨Ù‚ Ø¯Ù‚ÙŠÙ‚")
total_pages = (len(books) - 1) // BOOKS_PER_PAGE + 1 if books else 1

start = page * BOOKS_PER_PAGE
end = start + BOOKS_PER_PAGE
current_books = books[start:end]

if "Ø¨Ø­Ø« Ù…ÙˆØ³Ø¹" in search_stage:
stage_note = "âš ï¸ Ù†ØªØ§Ø¦Ø¬ Ø¨Ø­Ø« Ù…ÙˆØ³Ø¹ (Ø¨Ø­Ø«Ù†Ø§ Ø¨Ø§Ù„Ø¬Ø°ÙˆØ± ÙˆØ§Ù„Ù…Ø±Ø§Ø¯ÙØ§Øª)"
elif "ØªØ·Ø§Ø¨Ù‚ Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª" in search_stage:
stage_note = "âœ… Ù†ØªØ§Ø¦Ø¬ Ø¯Ù„Ø§Ù„ÙŠØ© (ØªØ·Ø§Ø¨Ù‚ Ø¬Ù…ÙŠØ¹ ÙƒÙ„Ù…Ø§ØªÙƒ Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©)"
elif "ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ø¹Ø¨Ø§Ø±Ø©" in search_stage:
stage_note = "ğŸ” ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ø¹Ø¨Ø§Ø±Ø© Ø§Ù„ÙƒØ§Ù…Ù„"
else:
stage_note = "âœ… Ù†ØªØ§Ø¦Ø¬ Ù…Ø·Ø§Ø¨Ù‚Ø©"

text = f"ğŸ“š Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ({len(books)} ÙƒØªØ§Ø¨)\n{stage_note}\nØ§Ù„ØµÙØ­Ø© {page + 1} Ù…Ù† {total_pages}\n\n"
keyboard = []

for b in current_books:
if not b.get("file_name") or not b.get("file_id"):
continue
key = hashlib.md5(str(b["file_id"]).encode()).hexdigest()[:16]
context.bot_data[f"file_{key}"] = b["file_id"]
keyboard.append([InlineKeyboardButton(f"{b['file_name']}", callback_data=f"file:{key}")])

nav_buttons = []
if page > 0:
nav_buttons.append(InlineKeyboardButton("â¬…ï¸ Ø§Ù„Ø³Ø§Ø¨Ù‚", callback_data="prev_page"))
if end < len(books):
nav_buttons.append(InlineKeyboardButton("Ø§Ù„ØªØ§Ù„ÙŠ â¡ï¸", callback_data="next_page"))
if nav_buttons:
keyboard.append(nav_buttons)

if context.user_data.get("is_index", False) or include_index_home:
keyboard.append([InlineKeyboardButton("ğŸ  Ø§Ù„Ø¹ÙˆØ¯Ø© Ù„Ù„ÙÙ‡Ø±Ø³", callback_data="home_index")])

reply_markup = InlineKeyboardMarkup(keyboard)
if update.message:
await update.message.reply_text(text, reply_markup=reply_markup)
elif update.callback_query:
await update.callback_query.message.edit_text(text, reply_markup=reply_markup)

=========================

Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø°ÙƒÙŠ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ù…Ø±Ø§Ø­Ù„ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ

=========================

async def search_books(update, context: ContextTypes.DEFAULT_TYPE):
if update.effective_chat.type != "private":
return

query = update.message.text.strip()
if not query:
return

conn = context.bot_data.get("db_conn")
if not conn:
await update.message.reply_text("âŒ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± Ù…ØªØµÙ„Ø© Ø­Ø§Ù„ÙŠØ§Ù‹.")
return

normalized_query = normalize_text(query)
clean_query = remove_common_words(query)
words_in_query = normalize_text(clean_query).split()
keywords = [w for w in words_in_query if w not in ARABIC_STOP_WORDS and len(w) >= 2]
expanded_keywords = expand_keywords_with_synonyms(keywords)
stemmed_keywords = [light_stem(k) for k in expanded_keywords]

context.user_data["last_query"] = normalized_query
context.user_data["last_keywords"] = keywords

ts_query_stemmed = ' & '.join(stemmed_keywords) if stemmed_keywords else ''
ts_query_expanded = ' | '.join(expanded_keywords) if expanded_keywords else ''
final_ts_query = f"({ts_query_stemmed}) | ({ts_query_expanded})" if ts_query_stemmed else ts_query_expanded

books = []
search_stage = "Ø¨Ø¯ÙˆÙ† Ù†ØªØ§Ø¦Ø¬"

if len(normalized_query.split()) > 2:
try:
books = await conn.fetch("""
SELECT id, file_id, file_name, uploaded_at
FROM books
WHERE file_name ILIKE $1
LIMIT 50;
""", f"%{normalized_query}%")
if books:
search_stage = "ğŸ” ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ø¹Ø¨Ø§Ø±Ø© Ø§Ù„ÙƒØ§Ù…Ù„"
except:
pass

if not books and final_ts_query:
try:
books = await conn.fetch("""
SELECT id, file_id, file_name, uploaded_at
FROM books
WHERE to_tsvector('arabic', file_name) @@ to_tsquery('arabic', $1)
ORDER BY ts_rank(to_tsvector('arabic', file_name), to_tsquery('arabic', $1)) DESC
LIMIT 400;
""", final_ts_query)
if books:
search_stage = "â­ Ø¨Ø­Ø« Ø¯Ù„Ø§Ù„ÙŠ (FTS + Ø§Ù„Ø¬Ø°ÙˆØ± Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©)"
except:
pass

if not books and stemmed_keywords:
try:
all_keywords_query = ' & '.join(stemmed_keywords)
books = await conn.fetch("""
SELECT id, file_id, file_name, uploaded_at
FROM books
WHERE to_tsvector('arabic', file_name) @@ to_tsquery('arabic', $1)
ORDER BY ts_rank(to_tsvector('arabic', file_name), to_tsquery('arabic', $1)) DESC
LIMIT 400;
""", all_keywords_query)
if books:
search_stage = "âœ… ØªØ·Ø§Ø¨Ù‚ Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© (AND FTS)"
except:
pass

if not books and stemmed_keywords:
try:
or_query = ' | '.join(stemmed_keywords)
books = await conn.fetch("""
SELECT id, file_id, file_name, uploaded_at
FROM books
WHERE to_tsvector('arabic', file_name) @@ to_tsquery('arabic', $1)
ORDER BY ts_rank(to_tsvector('arabic', file_name), to_tsquery('arabic', $1)) DESC
LIMIT 400;
""", or_query)
if books:
search_stage = "âš ï¸ Ø¨Ø­Ø« ÙˆØ§Ø³Ø¹ (OR Keywords FTS)"
except:
pass

if not books:
await send_search_suggestions(update, context)
context.user_data["search_results"] = []
context.user_data["current_page"] = 0
return

context.user_data["search_results"] = [dict(b) for b in books]
context.user_data["current_page"] = 0
context.user_data["search_stage"] = search_stage
await send_books_page(update, context)

==========================

Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø£Ø²Ø±Ø§Ø± Ø§Ù„ÙƒØªØ¨ ÙˆØ§Ù„ÙÙ‡Ø±Ø³

==========================

async def handle_callbacks(update, context: ContextTypes.DEFAULT_TYPE):
query = update.callback_query
await query.answer()
data = query.data

if data.startswith("file:"):
key = data.split(":")[1]
file_id = context.bot_data.get(f"file_{key}")
if file_id:
caption = "ğŸ“– ØªÙ… Ø§Ù„ØªÙ†Ø²ÙŠÙ„ Ø¨ÙˆØ§Ø³Ø·Ø© @boooksfree1bot"
share_button = InlineKeyboardMarkup([
[InlineKeyboardButton("Ø´Ø§Ø±Ùƒ Ø§Ù„Ø¨ÙˆØª Ù…Ø¹ Ø£ØµØ¯Ù‚Ø§Ø¦Ùƒ", switch_inline_query="")]
])
await query.message.reply_document(document=file_id, caption=caption, reply_markup=share_button)
else:
await query.message.reply_text("âŒ Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…ØªÙˆÙØ± Ø­Ø§Ù„ÙŠØ§Ù‹.")

elif data == "next_page":
context.user_data["current_page"] += 1
await send_books_page(update, context)

elif data == "prev_page":
context.user_data["current_page"] -= 1
await send_books_page(update, context)

elif data in ("home_index", "show_index"):
from index_handler import show_index
await show_index(update, context)
