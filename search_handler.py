import hashlib
from telegram import InlineKeyboardButton, InlineKeyboardMarkup
from telegram.ext import ContextTypes
import re
from typing import List, Dict, Any
import os
import math

BOOKS_PER_PAGE = 10

# -----------------------------
# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ø´Ø±Ù
# -----------------------------
try:
    ADMIN_USER_ID = int(os.getenv("ADMIN_ID", "0"))
except ValueError:
    ADMIN_USER_ID = 0
    print("âš ï¸ ADMIN_ID environment variable is not valid.")

# -----------------------------
# Ù…Ø¹Ù„Ù…Ø§Øª BM25 ÙˆØ®ÙŠØ§Ø±Ø§Øª Ø§Ù„Ø¨Ø­Ø«
# -----------------------------
BM25_K1 = 1.2
BM25_B = 0.75
MIN_SCORE = 1.0        # Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù„Ù„Ù†Ù‚Ø·Ø© Ù„ÙŠÙØ¹Ø±Ø¶ Ø§Ù„ÙƒØªØ§Ø¨ (ÙŠÙ…ÙƒÙ†Ùƒ ØªØºÙŠÙŠØ±Ù‡)
MAX_FETCH = 1000       # Ø£Ù‚ØµÙ‰ Ø¹Ø¯Ø¯ Ø³Ø¬Ù„Ø§Øª Ù†Ø¬Ù„Ø¨Ù‡Ø§ Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…Ø­Ù„ÙŠ (Ù„Ø­Ù…Ø§ÙŠØ© Ø§Ù„Ø£Ø¯Ø§Ø¡)

# -----------------------------
# Ù‚Ø§Ù…ÙˆØ³ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¬Ù…Ø¹ Ø¥Ù„Ù‰ Ø§Ù„Ù…ÙØ±Ø¯
# -----------------------------
WORD_MAP = {
    "Ø±ÙˆØ§ÙŠØ§Øª": "Ø±ÙˆØ§ÙŠØ©",
    "ÙƒØªØ¨": "ÙƒØªØ§Ø¨",
    "Ù…Ø¬Ù„Ø§Øª": "Ù…Ø¬Ù„Ø©",
    "Ù‚ØµØµ": "Ù‚ØµØ©"
}

# -----------------------------
# Ø¯ÙˆØ§Ù„ Ø§Ù„ØªØ·Ø¨ÙŠØ¹ ÙˆØ§Ù„ØªÙ†Ø¸ÙŠÙ
# -----------------------------
def normalize_text(text: str) -> str:
    if not text:
        return ""
    text = text.lower().replace("_", " ")
    text = text.replace("Ø£", "Ø§").replace("Ø¥", "Ø§").replace("Ø¢", "Ø§")
    text = text.replace("Ù‰", "ÙŠ").replace("Ù‡", "Ø©")
    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¬Ù…Ø¹ Ø¥Ù„Ù‰ Ù…ÙØ±Ø¯ Ù„Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ù‚Ø§Ù…ÙˆØ³
    words = text.split()
    normalized_words = [WORD_MAP.get(w, w) for w in words]
    return " ".join(normalized_words)

def remove_common_words(text: str) -> str:
    if not text:
        return ""
    for word in ["ÙƒØªØ§Ø¨", "Ø±ÙˆØ§ÙŠØ©", "Ù†Ø³Ø®Ø©", "Ù…Ø¬Ù…ÙˆØ¹Ø©", "Ù…Ø¬Ù„Ø¯", "Ø¬Ø²Ø¡"]:
        text = text.replace(word, "")
    return text.strip()

def extract_keywords(text: str) -> List[str]:
    if not text:
        return []
    clean_text = re.sub(r'[^\w\s]', '', text)
    words = clean_text.split()
    return [w for w in words if len(w) >= 2]  # Ø¯Ø¹Ù… Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù‚ØµÙŠØ±Ø©

def get_db_safe_query(normalized_query: str) -> str:
    return normalized_query.replace("'", "''")

# -----------------------------
# ØªÙˆØ³ÙŠØ¹ Ø§Ù„Ø¬Ø°Ø± (Root Expansion) -- Ù…Ø®ØªØµØ± ÙˆØ®ÙÙŠÙ
# -----------------------------
def expand_root(word: str) -> List[str]:
    variations = set()
    word = normalize_text(word)
    variations.add(word)
    # Ø¨Ø¹Ø¶ Ø§Ù„Ù„ÙˆØ§Ø­Ù‚ Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© Ù„Ù„ØªÙˆØ³ÙŠØ¹ Ø§Ù„Ø®ÙÙŠÙ
    suffixes = ["ÙŠØ©", "ÙŠ", "ÙˆÙ†", "Ø§Øª", "Ø§Ù†", "ÙŠÙ†", "Ø§ØªÙŠ"]
    for suf in suffixes:
        if word.endswith(suf) and len(word) > len(suf):
            variations.add(word[:-len(suf)])
    if word.startswith("Ø§Ù„") and len(word) > 2:
        variations.add(word[2:])
    # Ø¥Ø±Ø¬Ø§Ø¹ ØªÙ†ÙˆØ¹Ø§Øª Ù‚ØµÙŠØ±Ø© Ù„ØªØ·Ø§Ø¨Ù‚ Ø£ÙØ¶Ù„ Ù„ÙƒÙ† Ù„ÙŠØ³ Ù…Ø¨Ø§Ù„ØºÙ‹Ø§ (Ù„Ø­Ù…Ø§ÙŠØ© Ø§Ù„Ø£Ø¯Ø§Ø¡)
    return list(variations)

# -----------------------------
# Ø¯Ø§Ù„Ø© BM25 Ø®ÙÙŠÙØ© ØªØ¹Ù…Ù„ Ø¹Ù„Ù‰ Ø¹Ù†ÙˆØ§Ù† Ø§Ù„ÙƒØªØ§Ø¨ ÙÙ‚Ø·
# -----------------------------
def compute_bm25_for_corpus(corpus_titles: List[str], keywords: List[str]) -> Dict[int, float]:
    """
    Ø­Ø³Ø§Ø¨ Ù†Ù‚Ø§Ø· BM25 Ù„ÙƒÙ„ Ù…Ø³ØªÙ†Ø¯ (index ÙÙŠ corpus_titles).
    Ù†ÙØ³ØªØ®Ø¯Ù… Ø­ØµØµ Ù…Ø­Ù„ÙŠØ© (tf Ø¯Ø§Ø®Ù„ Ø§Ù„Ø¹Ù†ÙˆØ§Ù†ØŒ df = Ø¹Ø¯Ø¯ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø§Ù„Ù…ØµØ·Ù„Ø­).
    """
    N = len(corpus_titles)
    if N == 0:
        return {}

    # Ù†ÙØµÙ„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ù„ÙƒÙ„ Ø¹Ù†ÙˆØ§Ù† ÙˆÙ†Ø­Ø³Ø¨ Ø§Ù„Ø·ÙˆÙ„
    tokenized = [normalize_text(title).split() for title in corpus_titles]
    doc_lens = [len(toks) for toks in tokenized]
    avgdl = sum(doc_lens) / N if N > 0 else 0.0

    # Ø­Ø³Ø§Ø¨ df Ù„ÙƒÙ„ ÙƒÙ„Ù…Ø© ÙÙŠ keywords (Ø¯Ø§Ø®Ù„ Ù‡Ø°Ù‡ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† Ø§Ù„Ù…Ø¬Ù„ÙˆØ¨Ø©)
    df: Dict[str, int] = {}
    # Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© Ù†ÙˆØ³Ø¹ Ø¬Ø°Ø± ÙƒÙ„ ÙƒÙ„Ù…Ø© ÙˆÙ†Ø¹ØªØ¨Ø± Ø£ÙŠ ØªØ·Ø§Ø¨Ù‚ Ù…Ø¹ Ø£ÙŠ ØªÙ†ÙˆÙŠØ¹Ø© ÙƒÙˆØ¬ÙˆØ¯
    for kw in keywords:
        kw_roots = set(expand_root(kw))
        cnt = 0
        for toks in tokenized:
            found = False
            for t in toks:
                if any(root == t or t.startswith(root) for root in kw_roots):
                    found = True
                    break
            if found:
                cnt += 1
        df[kw] = max(1, cnt)  # ØªØ¬Ù†Ø¨ Ø§Ù„ØµÙØ± Ù„ØªÙØ§Ø¯ÙŠ Ø§Ù„Ù‚Ø³Ù…Ø© Ø¹Ù„Ù‰ ØµÙØ±

    # Ø­Ø³Ø§Ø¨ bm25 Ù„ÙƒÙ„ ÙˆØ«ÙŠÙ‚Ø©
    scores: Dict[int, float] = {}
    for idx, toks in enumerate(tokenized):
        score = 0.0
        dl = doc_lens[idx] if doc_lens[idx] > 0 else 1
        for kw in keywords:
            kw_roots = set(expand_root(kw))
            # tf = Ø¹Ø¯Ø¯ Ù…Ø±Ø§Øª Ø¸Ù‡ÙˆØ± Ø£ÙŠ Ù…Ù† ØªÙ†ÙˆÙŠØ¹Ø§Øª Ø§Ù„Ø¬Ø°Ø± ÙÙŠ Ø§Ù„Ù…Ø³ØªÙ†Ø¯
            tf = 0
            for t in toks:
                if any(root == t or t.startswith(root) for root in kw_roots):
                    tf += 1
            if tf == 0:
                continue
            # idf ØªÙ‚Ø±ÙŠØ¨ÙŠ
            idf = math.log((N - df.get(kw, 1) + 0.5) / (df.get(kw, 1) + 0.5) + 1)
            denom = tf + BM25_K1 * (1 - BM25_B + BM25_B * (dl / avgdl)) if avgdl > 0 else tf + BM25_K1
            score += idf * ((tf * (BM25_K1 + 1)) / denom)
        scores[idx] = score
    return scores

# -----------------------------
# Ø¯Ø§Ù„Ø© Ø§Ù„ØªÙ‚ÙŠÙŠÙ… (Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©) - Ø®ÙÙŠÙØ©
# -----------------------------
def heuristic_score(title: str, keywords: List[str], normalized_query: str) -> float:
    """Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„ÙˆØ²Ù†ÙŠ Ø§Ù„Ù‚Ø¯ÙŠÙ… Ø§Ù„Ù…Ø®ØªØµØ± (Ù…Ø·Ø§Ø¨Ù‚Ø§Øª Ø­Ø±ÙÙŠØ©ØŒ Ø¨Ø¯Ø§ÙŠØ© ÙƒÙ„Ù…Ø©ØŒ Ø§Ø­ØªÙˆØ§Ø¡)."""
    score = 0.0
    name = normalize_text(title)
    # Ø§Ù„ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ø­Ø±ÙÙŠ Ø§Ù„ÙƒØ§Ù…Ù„ Ø£Ùˆ Ø¹Ø¨Ø§Ø±Ø©
    if normalized_query == name:
        score += 50
    elif normalized_query in name:
        score += 20

    words_in_name = name.split()
    for kw in keywords:
        roots = expand_root(kw)
        # Ù†Ù‚Ø§Ø· Ø¹Ù†Ø¯ Ø§Ù„ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ù…Ø¨Ø§Ø´Ø± Ø£Ùˆ Ø¨Ø¯Ø§ÙŠØ© Ø§Ù„ÙƒÙ„Ù…Ø©
        for root in roots:
            for w in words_in_name:
                if w.startswith(root):
                    score += 6
                elif root in w:
                    score += 4
        # Ø²ÙŠØ§Ø¯Ø© Ù†Ù‚Ø§Ø· Ø¥Ø°Ø§ ØªØ·Ø§Ø¨Ù‚Øª Ø§Ù„ÙƒÙ„Ù…Ø© Ø¨Ø§Ù„ÙƒØ§Ù…Ù„
        if kw in name:
            score += 10
    return score

# -----------------------------
# Ø¯Ù…Ø¬ Ø§Ù„Ø¯Ø±Ø¬Ø§Øª: bm25 + heuristic
# -----------------------------
def combined_score(bm25_val: float, heur: float, weight_bm25: float = 0.7) -> float:
    return weight_bm25 * bm25_val + (1 - weight_bm25) * heur

# -----------------------------
# Ø¥Ø´Ø¹Ø§Ø± Ø§Ù„Ù…Ø´Ø±Ù
# -----------------------------
async def notify_admin_search(context: ContextTypes.DEFAULT_TYPE, username: str, query: str, found: bool):
    if ADMIN_USER_ID == 0:
        return
    bot = context.bot
    status_text = "âœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬" if found else "âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬"
    username_text = f"@{username}" if username else "(Ø¨Ø¯ÙˆÙ† ÙŠÙˆØ²Ø±)"
    message = f"ğŸ”” Ù‚Ø§Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… {username_text} Ø¨Ø§Ù„Ø¨Ø­Ø« Ø¹Ù†:\n`{query}`\nØ§Ù„Ø­Ø§Ù„Ø©: {status_text}"
    try:
        await bot.send_message(ADMIN_USER_ID, message, parse_mode='Markdown')
    except Exception as e:
        print(f"Failed to notify admin: {e}")

# -----------------------------
# Ø¥Ø±Ø³Ø§Ù„ ØµÙØ­Ø© Ø§Ù„ÙƒØªØ¨ (ÙƒÙ…Ø§ ÙƒØ§Ù†)
# -----------------------------
async def send_books_page(update, context: ContextTypes.DEFAULT_TYPE):
    books = context.user_data.get("search_results", [])
    page = context.user_data.get("current_page", 0)
    search_stage = context.user_data.get("search_stage", "ØªØ·Ø§Ø¨Ù‚ Ø¯Ù‚ÙŠÙ‚")
    total_pages = (len(books) - 1) // BOOKS_PER_PAGE + 1 if books else 1

    start = page * BOOKS_PER_PAGE
    end = start + BOOKS_PER_PAGE
    current_books = books[start:end]

    if "Ø¨Ø­Ø« Ù…ÙˆØ³Ø¹" in search_stage:
        stage_note = "âš ï¸ Ù†ØªØ§Ø¦Ø¬ Ø¨Ø­Ø« Ù…ÙˆØ³Ø¹ (Ø¨Ø­Ø«Ù†Ø§ Ø¨Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©)"
    elif "ØªØ·Ø§Ø¨Ù‚ Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª" in search_stage:
        stage_note = "âœ… Ù†ØªØ§Ø¦Ø¬ Ø¯Ù„Ø§Ù„ÙŠØ© (ØªØ·Ø§Ø¨Ù‚ Ø¬Ù…ÙŠØ¹ ÙƒÙ„Ù…Ø§ØªÙƒ)"
    else:
        stage_note = "âœ… Ù†ØªØ§Ø¦Ø¬ Ù…Ø·Ø§Ø¨Ù‚Ø© (ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ø¹Ø¨Ø§Ø±Ø© ÙƒØ§Ù…Ù„Ø©)"

    text = f"ğŸ“š Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ({len(books)} ÙƒØªØ§Ø¨)\n{stage_note}\nØ§Ù„ØµÙØ­Ø© {page + 1} Ù…Ù† {total_pages}\n\n"
    keyboard = []

    for b in current_books:
        if not b.get("file_name") or not b.get("file_id"):
            continue
        key = hashlib.md5(b["file_id"].encode()).hexdigest()[:16]
        context.bot_data[f"file_{key}"] = b["file_id"]
        keyboard.append([InlineKeyboardButton(f"ğŸ“˜ {b['file_name']}", callback_data=f"file:{key}")])

    nav_buttons = []
    if page > 0:
        nav_buttons.append(InlineKeyboardButton("â¬…ï¸ Ø§Ù„Ø³Ø§Ø¨Ù‚", callback_data="prev_page"))
    if end < len(books):
        nav_buttons.append(InlineKeyboardButton("Ø§Ù„ØªØ§Ù„ÙŠ â¡ï¸", callback_data="next_page"))
    if nav_buttons:
        keyboard.append(nav_buttons)

    reply_markup = InlineKeyboardMarkup(keyboard)
    if update.message:
        await update.message.reply_text(text, reply_markup=reply_markup)
    elif update.callback_query:
        await update.callback_query.message.edit_text(text, reply_markup=reply_markup)

# -----------------------------
# Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ø­Ø³Ù† (Ø§Ù„Ù…Ø¯Ù…Ø¬)
# -----------------------------
async def search_books(update, context: ContextTypes.DEFAULT_TYPE):
    if update.effective_chat.type != "private":
        return

    query = update.message.text.strip()
    if not query:
        return

    conn = context.bot_data.get("db_conn")
    if not conn:
        await update.message.reply_text("âŒ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± Ù…ØªØµÙ„Ø© Ø­Ø§Ù„ÙŠØ§Ù‹.")
        return

    # ØªØ·Ø¨ÙŠØ¹ ÙˆØ§Ø³ØªØ®Ø±Ø¬ ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¨Ø­Ø«
    normalized_query = normalize_text(remove_common_words(query))
    keywords = extract_keywords(normalized_query)
    # Ø¥Ø°Ø§ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙƒØªØ¨ Ø¹Ø¨Ø§Ø±Ø© ÙƒØ§Ù…Ù„Ø© Ø·ÙˆÙŠÙ„Ø© Ù†Ø¶ÙŠÙÙ‡Ø§ ÙƒÙ…Ø±Ø§Ø¯Ù Ù„Ù„ÙƒÙ„Ù…Ø§Øª
    if normalized_query and normalized_query not in keywords:
        # Ù„Ùˆ Ø§Ù„Ø¹Ø¨Ø§Ø±Ø© Ø£ÙƒØ«Ø± Ù…Ù† ÙƒÙ„Ù…Ø©ØŒ Ø¶Ù…Ù‘Ù†Ù‡Ø§ ÙƒÙ€ keyword ÙˆØ§Ø­Ø¯ Ù„Ø²ÙŠØ§Ø¯Ø© Ø¯Ù‚Ø© Ø§Ù„Ø¹Ø¨Ø§Ø±Ø©
        if len(normalized_query.split()) > 1:
            keywords.insert(0, normalized_query)

    context.user_data["last_query"] = normalized_query
    context.user_data["last_keywords"] = keywords

    if not keywords:
        await update.message.reply_text("âŒ Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙƒÙ„Ù…Ø§Øª Ù‚ØµÙŠØ±Ø© Ø¬Ø¯Ù‹Ø§.")
        return

    # Ø§Ù„Ù…Ø±Ø­Ù„Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰: Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø³Ø±ÙŠØ¹ Ù…Ø­Ø¯ÙˆØ¯ Ù„ØªØµÙÙŠØ© Ø§Ù„Ù…Ø±Ø´Ø­ÙŠÙ†
    try:
        or_conditions = " OR ".join([f"LOWER(file_name) LIKE '%{get_db_safe_query(k)}%'" for k in keywords])
        books = await conn.fetch(f"""
            SELECT id, file_id, file_name, uploaded_at
            FROM books
            WHERE {or_conditions}
            ORDER BY uploaded_at DESC
            LIMIT {MAX_FETCH};
        """)
    except Exception as e:
        await update.message.reply_text("âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø«.")
        return

    # Ø¥Ù† Ù„Ù… Ù†Ø¬Ø¯ Ø´ÙŠØ¡ØŒ Ù†Ø¬Ø±Ø¨ Ø¨Ø­Ø« Ø£ÙˆØ³Ø¹ (Ù†ÙØ³ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ù„ÙƒÙ† Ø¨Ø¯ÙˆÙ† LIMIT Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ø®Ø·Ø± Ù„Ø°Ø§ Ù†Ø­Ø§ÙØ¸ Ø¹Ù„Ù‰ LIMIT)
    if not books:
        try:
            books = await conn.fetch(f"""
                SELECT id, file_id, file_name, uploaded_at
                FROM books
                WHERE {" OR ".join([f"LOWER(file_name) LIKE '%{get_db_safe_query(k)}%'" for k in keywords])}
                ORDER BY uploaded_at DESC
                LIMIT {MAX_FETCH};
            """)
        except Exception as e:
            await update.message.reply_text("âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø«.")
            return

    found_results = bool(books)

    # Ù†Ø­Ø¶Ø± Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† ÙƒÙ‚Ø§Ø¦Ù…Ø© Ù„Ù„Ù…ØµØ­Ù BM25 Ø§Ù„Ù…Ø­Ù„ÙŠ
    corpus_titles = [b['file_name'] for b in books]
    bm25_scores = compute_bm25_for_corpus(corpus_titles, keywords)

    # Ø§Ù„Ø¢Ù† Ù†Ø­Ø³Ø¨ Ø§Ù„Ø¯Ø±Ø¬Ø© Ø§Ù„Ù…Ø±ÙƒØ¨Ø© Ù„ÙƒÙ„ ÙƒØªØ§Ø¨
    scored_books = []
    for idx, book in enumerate(books):
        heur = heuristic_score(book['file_name'], keywords, normalized_query)
        bm25_val = bm25_scores.get(idx, 0.0)
        total = combined_score(bm25_val, heur, weight_bm25=0.7)
        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø­Ø¯ MIN_SCORE Ù„Ø¥Ø®Ø±Ø§Ø¬ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¶Ø¹ÙŠÙØ©
        if total >= MIN_SCORE:
            book_dict = dict(book)
            book_dict['score'] = total
            scored_books.append(book_dict)

    # Ø±ØªØ¨ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù†Ù‡Ø§Ø¦ÙŠÙ‹Ø§
    scored_books.sort(key=lambda b: (b['score'], b['uploaded_at']), reverse=True)

    await notify_admin_search(context, update.effective_user.username, query, bool(scored_books))

    if not scored_books:
        keyboard = InlineKeyboardMarkup([[InlineKeyboardButton("ğŸ” Ø¨Ø­Ø« Ø¹Ù† ÙƒØªØ¨ Ù…Ø´Ø§Ø¨Ù‡Ø©", callback_data="search_similar")]])
        await update.message.reply_text(f"âŒ Ù„Ù… Ø£Ø¬Ø¯ Ø£ÙŠ ÙƒØªØ¨ Ù…Ø·Ø§Ø¨Ù‚Ø© Ù„Ù„Ø¨Ø­Ø«: {query}\nÙŠÙ…ÙƒÙ†Ùƒ ØªØ¬Ø±Ø¨Ø© Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙƒØªØ¨ Ù…Ø´Ø§Ø¨Ù‡Ø©:", reply_markup=keyboard)
        context.user_data["search_results"] = []
        context.user_data["current_page"] = 0
        return

    context.user_data["search_results"] = scored_books
    context.user_data["current_page"] = 0
    context.user_data["search_stage"] = "Ø¨Ø­Ø« Ù…Ø­Ø³Ù† (BM25+Ø¬Ø°Ø±)"
    await send_books_page(update, context)

# -----------------------------
# Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙƒØªØ¨ Ù…Ø´Ø§Ø¨Ù‡Ø© (ÙŠØ¹ØªÙ…Ø¯ Ù†ÙØ³ Ø§Ù„Ù…ÙŠÙƒØ§Ù†ÙŠÙƒ)
# -----------------------------
async def search_similar_books(update, context: ContextTypes.DEFAULT_TYPE):
    conn = context.bot_data.get("db_conn")
    keywords = context.user_data.get("last_keywords")
    if not keywords or not conn:
        await update.callback_query.message.reply_text("âŒ Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù…ÙˆØ¶ÙˆØ¹ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù†Ù‡.")
        return

    try:
        or_conditions = " OR ".join([f"LOWER(file_name) LIKE '%{get_db_safe_query(k)}%'" for k in keywords])
        books = await conn.fetch(f"""
            SELECT id, file_id, file_name, uploaded_at
            FROM books
            WHERE {or_conditions}
            ORDER BY uploaded_at DESC
            LIMIT {MAX_FETCH};
        """)
    except Exception as e:
        await update.callback_query.message.reply_text("âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙƒØªØ¨ Ù…Ø´Ø§Ø¨Ù‡Ø©.")
        return

    corpus_titles = [b['file_name'] for b in books]
    bm25_scores = compute_bm25_for_corpus(corpus_titles, keywords)

    scored_books = []
    for idx, book in enumerate(books):
        heur = heuristic_score(book['file_name'], keywords, context.user_data.get("last_query", ""))
        bm25_val = bm25_scores.get(idx, 0.0)
        total = combined_score(bm25_val, heur, weight_bm25=0.7)
        if total >= MIN_SCORE:
            book_dict = dict(book)
            book_dict['score'] = total
            scored_books.append(book_dict)

    scored_books.sort(key=lambda b: (b['score'], b['uploaded_at']), reverse=True)
    if not scored_books:
        await update.callback_query.message.reply_text("âŒ Ù„Ù… Ø£Ø¬Ø¯ ÙƒØªØ¨ Ù…Ø´Ø§Ø¨Ù‡Ø©.")
        return

    context.user_data["search_results"] = scored_books
    context.user_data["current_page"] = 0
    context.user_data["search_stage"] = "Ø¨Ø­Ø« Ù…ÙˆØ³Ø¹ (Ù…Ø´Ø§Ø¨Ù‡)"
    await send_books_page(update, context)

# -----------------------------
# Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø£Ø²Ø±Ø§Ø± Ø§Ù„ÙƒØªØ¨ ÙˆØ§Ù„Ù…Ø´Ø§Ø±ÙƒØ©
# -----------------------------
async def handle_callbacks(update, context: ContextTypes.DEFAULT_TYPE):
    query = update.callback_query
    await query.answer()
    data = query.data

    if data.startswith("file:"):
        key = data.split(":")[1]
        file_id = context.bot_data.get(f"file_{key}")
        if file_id:
            caption = "ØªÙ… Ø§Ù„ØªÙ†Ø²ÙŠÙ„ Ø¨ÙˆØ§Ø³Ø·Ø© @boooksfree1bot"
            share_button = InlineKeyboardMarkup([
                [InlineKeyboardButton("ğŸ“¤ Ø´Ø§Ø±Ùƒ Ø§Ù„Ø¨ÙˆØª Ù…Ø¹ Ø£ØµØ¯Ù‚Ø§Ø¦Ùƒ", switch_inline_query="")]
            ])
            await query.message.reply_document(document=file_id, caption=caption, reply_markup=share_button)
        else:
            await query.message.reply_text("âŒ Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…ØªÙˆÙØ± Ø­Ø§Ù„ÙŠØ§Ù‹.")
    elif data == "next_page":
        context.user_data["current_page"] += 1
        await send_books_page(update, context)
    elif data == "prev_page":
        context.user_data["current_page"] -= 1
        await send_books_page(update, context)
    elif data == "search_similar":
        await search_similar_books(update, context)
