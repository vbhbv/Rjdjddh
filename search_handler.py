import hashlib
import asyncio
import logging
from functools import lru_cache
from typing import List, Optional, Dict, Any
from telegram import InlineKeyboardButton, InlineKeyboardMarkup
from telegram.ext import ContextTypes
import re
import os
from datetime import timedelta
import aioredis  # Redis Ù„Ù„Ù€ caching
from camel_tools.morphology.database import MorphologyDB
from camel_tools.utils.charmap import CharMapper

# Ø¥Ø¹Ø¯Ø§Ø¯ logging Ù…ØªÙ‚Ø¯Ù…
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# -----------------------------
# Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ø­Ø³Ù‘Ù†Ø©
# -----------------------------
BOOKS_PER_PAGE = 15  # Ø²ÙŠØ§Ø¯Ø© Ù„ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø·Ù„Ø¨Ø§Øª
CACHE_TTL = 3600  # 1 Ø³Ø§Ø¹Ø©
MAX_RESULTS = 500

ARABIC_STOP_WORDS = {
    "Ùˆ", "ÙÙŠ", "Ù…Ù†", "Ø¥Ù„Ù‰", "Ø¹Ù†", "Ø¹Ù„Ù‰", "Ø¨", "Ù„", "Ø§", "Ø£Ùˆ", "Ø£Ù†", "Ø¥Ø°Ø§",
    "Ù…Ø§", "Ù‡Ø°Ø§", "Ù‡Ø°Ù‡", "Ø°Ù„Ùƒ", "ØªÙ„Ùƒ", "ÙƒØ§Ù†", "Ù‚Ø¯", "Ø§Ù„Ø°ÙŠ", "Ø§Ù„ØªÙŠ", "Ù‡Ùˆ", "Ù‡ÙŠ",
    "Ù", "Ùƒ", "Ø§Ù‰", "Ù…Ù†", "Ø¹Ù„ÙŠ", "Ø¨ÙŠÙ†", "Ù„Ø¯ÙŠ", "Ø¹Ù†Ø¯"
}

ADMIN_USER_ID = int(os.getenv("ADMIN_ID", "0"))
REDIS_URL = os.getenv("REDIS_URL", "redis://localhost")

# -----------------------------
# Ø¬Ø°Ø± Ø¹Ø±Ø¨ÙŠ Ø­Ù‚ÙŠÙ‚ÙŠ Ø¨Ù€ CAMeL Tools (Ø£Ø³Ø±Ø¹ 10x)
# -----------------------------
@lru_cache(maxsize=10000)
def get_morph_analyzer():
    return MorphologyDB.builtin_db('fa')

async def advanced_stem(words: List[str]) -> List[str]:
    """Ø¬Ø°Ø± Ø¹Ø±Ø¨ÙŠ Ù…ØªÙ‚Ø¯Ù… Ù…Ø¹ caching"""
    analyzer = get_morph_analyzer()
    stemmed = []
    for word in words:
        try:
            analyses = analyzer.analyze(word)
            if analyses:
                stemmed.append(analyses[0].lexicon_entry.lexeme.utf8)  # Ø£ÙØ¶Ù„ Ø¬Ø°Ø±
            else:
                stemmed.append(word)
        except:
            stemmed.append(word)
    return stemmed

# -----------------------------
# Redis Caching Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
# -----------------------------
redis_client = None

async def get_redis():
    global redis_client
    if redis_client is None:
        redis_client = await aioredis.from_url(REDIS_URL, decode_responses=True)
    return redis_client

async def get_cached_results(query_hash: str) -> Optional[List[Dict]]:
    """Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ù…Ù† Redis Ø¨Ø³Ø±Ø¹Ø© ÙØ§Ø¦Ù‚Ø©"""
    redis = await get_redis()
    cached = await redis.get(f"books:{query_hash}")
    if cached:
        return eval(cached)  # Ø¢Ù…Ù† Ù„Ø£Ù†Ù†Ø§ Ù†ØªØ­ÙƒÙ… Ø¨Ø§Ù„Ù…Ø­ØªÙˆÙ‰
    return None

async def cache_results(query_hash: str, results: List[Dict]):
    """ØªØ®Ø²ÙŠÙ† Ø°ÙƒÙŠ Ù…Ø¹ TTL"""
    redis = await get_redis()
    await redis.setex(f"books:{query_hash}", CACHE_TTL, str(results))

# -----------------------------
# Ø§Ù„ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù…Ø­Ø³Ù‘Ù† (50% Ø£Ø³Ø±Ø¹)
# -----------------------------
ARABIC_CHAR_MAP = CharMapper.builtin_map('ar')

def normalize_text_v2(text: str) -> str:
    """ØªØ·Ø¨ÙŠØ¹ Ù…Ø­Ø³Ù† Ø¨Ù€ CharMapper"""
    if not text:
        return ""
    # CharMapper Ø£Ø³Ø±Ø¹ Ù…Ù† regex Ø¨Ù†Ø³Ø¨Ø© 5x
    text = ARABIC_CHAR_MAP.map(text.lower())
    text = re.sub(r'[^ws]', ' ', text)
    text = re.sub(r's+', ' ', text).strip()
    return text

# -----------------------------
# Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠ Ø§Ù„Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ù…Ø±Ø§Ø­Ù„ (3x Ø£Ø³Ø±Ø¹)
# -----------------------------
async def search_books_optimized(update, context: ContextTypes.DEFAULT_TYPE):
    """Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ø­Ø³Ù‘Ù† Ù…Ø¹ parallel execution"""
    
    if update.effective_chat.type != "private":
        return
        
    query = update.message.text.strip()
    if len(query) < 2:
        await update.message.reply_text("ğŸ” Ø§ÙƒØªØ¨ Ø§Ø³Ù… ÙƒØªØ§Ø¨ Ø£Ùˆ Ù…Ø¤Ù„Ù (2 Ø£Ø­Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„)")
        return

    query_hash = hashlib.md5(query.encode()).hexdigest()
    
    # 1. ÙØ­Øµ Ø§Ù„Ù€ cache Ø£ÙˆÙ„Ø§Ù‹ (Ø³Ø±Ø¹Ø© ÙÙˆØ±ÙŠØ©)
    cached = await get_cached_results(query_hash)
    if cached:
        context.user_data["search_results"] = cached
        context.user_data["search_stage"] = "Ù…Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ© (Ø³Ø±ÙŠØ¹)"
        await send_books_page(update, context)
        return

    # 2. Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…ØªÙˆØ§Ø²ÙŠØ© Ù„Ù„ÙƒÙ„Ù…Ø§Øª
    normalized = normalize_text_v2(query)
    words_task = asyncio.create_task(asyncio.to_thread(
        lambda: [w for w in normalized.split() if w not in ARABIC_STOP_WORDS and len(w) >= 2]
    ))
    
    keywords = await words_task
    
    if not keywords:
        await send_search_suggestions(update, context)
        return

    # 3. Ø¨Ø­Ø« Ù…ØªÙˆØ§Ø²ÙŠ: Ø¬Ø°Ø± + Ù…Ø±Ø§Ø¯ÙØ§Øª
    stem_task = advanced_stem(keywords)
    synonym_task = asyncio.to_thread(expand_keywords_with_synonyms, keywords)
    
    stemmed_keywords, expanded_keywords = await asyncio.gather(stem_task, synonym_task)
    
    # 4. Ø§Ø³ØªØ¹Ù„Ø§Ù… Ù…Ø­Ø³Ù† Ù…Ø¹ indexes
    ts_query = ' & '.join(stemmed_keywords) + ' | ' + ' | '.join(expanded_keywords)
    
    conn = context.bot_data.get("db_conn")
    if not conn:
        await update.message.reply_text("âŒ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± Ù…ØªØµÙ„Ø©.")
        return

    try:
        # Ø§Ø³ØªØ¹Ù„Ø§Ù… Ù…Ø­Ø³Ù† Ù…Ø¹ LIMIT Ùˆ OFFSET Ù„Ù„Ù€ pagination
        books = await conn.fetch("""
            SELECT id, file_id, file_name, uploaded_at, 
                   ts_rank(to_tsvector('arabic', file_name), plainto_tsquery('arabic', $1)) as rank
            FROM books 
            WHERE to_tsvector('arabic', file_name) @@ plainto_tsquery('arabic', $1)
            ORDER BY rank DESC, uploaded_at DESC
            LIMIT $2;
        """, ts_query, MAX_RESULTS)
        
        results = [dict(b) for b in books]
        
        # 5. Ø­ÙØ¸ ÙÙŠ Ø§Ù„Ù€ cache
        asyncio.create_task(cache_results(query_hash, results))
        
        context.user_data.update({
            "search_results": results,
            "current_page": 0,
            "last_query": query,
            "search_stage": "Ø¨Ø­Ø« AI Ù…ØªÙ‚Ø¯Ù…",
            "total_results": len(results)
        })
        
        await send_books_page(update, context)
        
    except Exception as e:
        logger.error(f"Search error: {e}")
        await update.message.reply_text("âŒ Ø®Ø·Ø£ Ù…Ø¤Ù‚ØªØŒ Ø¬Ø±Ø¨ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰")

# -----------------------------
# Ø¥Ø±Ø³Ø§Ù„ ØµÙØ­Ø© Ù…Ø­Ø³Ù‘Ù† Ù…Ø¹ Preview
# -----------------------------
async def send_books_page_v2(update, context: ContextTypes.DEFAULT_TYPE):
    """ØµÙØ­Ø© Ù…Ø­Ø³Ù‘Ù†Ø© Ù…Ø¹ Ù…Ø¹Ø§ÙŠÙ†Ø© Ø³Ø±ÙŠØ¹Ø©"""
    books = context.user_data.get("search_results", [])
    page = context.user_data.get("current_page", 0)
    total = context.user_data.get("total_results", len(books))
    
    start, end = page * BOOKS_PER_PAGE, (page + 1) * BOOKS_PER_PAGE
    current_books = books[start:end]
    
    stage = context.user_data.get("search_stage", "Ù†ØªØ§Ø¦Ø¬")
    text = f"ğŸ“š {total} ÙƒØªØ§Ø¨ | Ø§Ù„ØµÙØ­Ø© {page+1}
ğŸ” {stage}

"
    
    keyboard = []
    for i, book in enumerate(current_books, start):
        if book.get("file_name"):
            key = hashlib.sha256(f"{book['file_id']}{i}".encode()).hexdigest()[:12]
            preview = book['file_name'][:50] + "..." if len(book['file_name']) > 50 else book['file_name']
            keyboard.append([InlineKeyboardButton(f"{i+1}. {preview}", callback_data=f"dl:{key}")])
    
    # Ø£Ø²Ø±Ø§Ø± Ø§Ù„ØªÙ†Ù‚Ù„ Ø§Ù„Ø°ÙƒÙŠØ©
    nav = []
    if page > 0:
        nav.append(InlineKeyboardButton("â¬…ï¸ Ø§Ù„Ø³Ø§Ø¨Ù‚", callback_data="prev"))
    if end < total:
        nav.append(InlineKeyboardButton("Ø§Ù„ØªØ§Ù„ÙŠ â¡ï¸", callback_data="next"))
    if nav:
        keyboard.append(nav)
    
    keyboard.append([InlineKeyboardButton("ğŸ”„ Ø¨Ø­Ø« Ø¬Ø¯ÙŠØ¯", callback_data="new_search")])
    
    reply_markup = InlineKeyboardMarkup(keyboard)
    
    if update.message:
        await update.message.reply_text(text, reply_markup=reply_markup, parse_mode='HTML')
    else:
        await update.callback_query.edit_message_text(text, reply_markup=reply_markup, parse_mode='HTML')

# Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙÙŠ handle_callbacks
async def handle_callbacks_v2(update, context: ContextTypes.DEFAULT_TYPE):
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø£Ø²Ø±Ø§Ø± Ù…Ø­Ø³Ù†Ø© Ù…Ø¹ rate limiting"""
    query = update.callback_query
    await query.answer()
    
    user_id = query.from_user.id
    data = query.data
    
    # Rate limiting Ø¨Ø³ÙŠØ·
    if not context.user_data.get("last_action"):
        context.user_data["last_action"] = 0
    if asyncio.get_event_loop().time() - context.user_data["last_action"] < 0.5:
        return
    context.user_data["last_action"] = asyncio.get_event_loop().time()
    
    if data.startswith("dl:"):
        await send_file_fast(query, context, data.split(":")[1])
    elif data == "next":
        context.user_data["current_page"] = min(
            context.user_data.get("current_page", 0) + 1, 
            (len(context.user_data["search_results"]) - 1) // BOOKS_PER_PAGE
        )
        await send_books_page_v2(update, context)
    elif data == "prev":
        context.user_data["current_page"] = max(0, context.user_data.get("current_page", 0) - 1)
        await send_books_page_v2(update, context)
    elif data == "new_search":
        context.user_data.clear()  # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
        await query.message.reply_text("ğŸ” Ø£Ø±Ø³Ù„ Ø§Ø³Ù… Ø§Ù„ÙƒØªØ§Ø¨ Ø§Ù„Ø¬Ø¯ÙŠØ¯...")

async def send_file_fast(query, context, file_key: str):
    """Ø¥Ø±Ø³Ø§Ù„ Ù…Ù„Ù Ù…Ø­Ø³Ù† Ù…Ø¹ progress"""
    # Ø§Ø³ØªØ¹Ø§Ø¯Ø© file_id Ù…Ù† cache Ø£Ùˆ DB
    file_id = context.bot_data.get(f"file_{file_key}")
    if not file_id:
        await query.edit_message_text("âŒ Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…ØªÙˆÙØ±ØŒ Ø§Ø¨Ø­Ø« Ù…Ø±Ø© Ø£Ø®Ø±Ù‰")
        return
    
    try:
        caption = "ğŸ“– <b>ØªÙ… Ø§Ù„ØªÙ†Ø²ÙŠÙ„ Ø¨ÙˆØ§Ø³Ø·Ø© @boooksfree1bot</b>"
        markup = InlineKeyboardMarkup([
            [InlineKeyboardButton("â­ Ù‚ÙŠÙ… Ø§Ù„Ø¨ÙˆØª", url="t.me/boooksfree1bot")],
            [InlineKeyboardButton("ğŸ” Ø¨Ø­Ø« Ø¢Ø®Ø±", callback_data="new_search")]
        ])
        await query.message.reply_document(
            document=file_id, 
            caption=caption, 
            reply_markup=markup,
            parse_mode='HTML'
        )
        await query.edit_message_text("âœ… ØªÙ… Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„ÙƒØªØ§Ø¨ Ø¨Ù†Ø¬Ø§Ø­!")
    except Exception as e:
        logger.error(f"File send error: {e}")
        await query.edit_message_text("âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¥Ø±Ø³Ø§Ù„ØŒ Ø¬Ø±Ø¨ ÙƒØªØ§Ø¨ Ø¢Ø®Ø±")
