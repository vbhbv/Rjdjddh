import hashlib
from telegram import InlineKeyboardButton, InlineKeyboardMarkup
from telegram.ext import ContextTypes
import re
from typing import List, Dict, Any
import os

# -----------------------------
# Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ÙˆÙ‚Ø§Ø¦Ù…Ø© Stop Words
# -----------------------------

BOOKS_PER_PAGE = 10

# Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ØªÙŠ ÙŠØ¬Ø¨ ØªØ¬Ø§Ù‡Ù„Ù‡Ø§ (Stop Words)
# Ù„Ù…Ù†Ø¹ ØªØ¶ÙŠÙŠÙ‚ Ø§Ù„Ø¨Ø­Ø« Ø¨Ù‡Ø§ ÙÙŠ Ø§Ù„Ù…Ø±Ø§Ø­Ù„ Ø§Ù„Ø£ÙˆÙ„Ù‰
ARABIC_STOP_WORDS = {
    "Ùˆ", "ÙÙŠ", "Ù…Ù†", "Ø¥Ù„Ù‰", "Ø¹Ù†", "Ø¹Ù„Ù‰", "Ø¨", "Ù„", "Ø§", "Ø£Ùˆ", "Ø£Ù†", "Ø¥Ø°Ø§",
    "Ù…Ø§", "Ù‡Ø°Ø§", "Ù‡Ø°Ù‡", "Ø°Ù„Ùƒ", "ØªÙ„Ùƒ", "ÙƒØ§Ù†", "Ù‚Ø¯", "Ø§Ù„Ø°ÙŠ", "Ø§Ù„ØªÙŠ", "Ù‡Ùˆ", "Ù‡ÙŠ",
    "Ù", "Ùƒ", "Ø§Ù‰"
}

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ø´Ø±Ù
try:
    ADMIN_USER_ID = int(os.getenv("ADMIN_ID", "0"))
except ValueError:
    ADMIN_USER_ID = 0
    print("âš ï¸ ADMIN_ID environment variable is not valid.")

# -----------------------------
# Ø¯ÙˆØ§Ù„ Ø§Ù„ØªØ·Ø¨ÙŠØ¹ ÙˆØ§Ù„ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…ÙØ­Ø³Ù‘Ù†Ø©
# -----------------------------
def normalize_text(text: str) -> str:
    """ÙŠÙˆØ­Ø¯ Ø§Ù„Ø­Ø±ÙˆÙ ÙˆÙŠØ²ÙŠÙ„ Ø§Ù„Ø­Ø±ÙƒØ§Øª ÙˆØ§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©."""
    if not text: return ""
    text = str(text).lower()
    text = text.replace("_", " ")
    text = text.replace("Ø£", "Ø§").replace("Ø¥", "Ø§").replace("Ø¢", "Ø§")
    text = text.replace("Ù‰", "ÙŠ").replace("Ø©", "Ù‡")
    text = text.replace("Ù€", "")
    text = re.sub(r"[Ù‹ÙŒÙÙÙÙ]", "", text)
    
    # ØªØ­Ø³ÙŠÙ†: Ù…Ø¹Ø§Ù„Ø¬Ø© Ø´Ø§Ù…Ù„Ø© Ù„Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ø¨ÙŠØ¶Ø§Ø¡ ÙˆØ§Ù„Ø±Ù…ÙˆØ²
    text = re.sub(r'[^\w\s]', ' ', text) # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø§Ù„Ø±Ù…ÙˆØ² Ø¨Ù…Ø³Ø§ÙØ©
    text = re.sub(r'\s+', ' ', text).strip() # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø© ÙˆØ§Ù„ØªÙ†Ø¸ÙŠÙ
    return text

def remove_common_words(text: str) -> str:
    """ÙŠØ²ÙŠÙ„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¯Ø§Ù„Ø© Ø¹Ù„Ù‰ Ù†ÙˆØ¹ Ø§Ù„Ù…Ù„Ù (ÙƒØªØ§Ø¨ØŒ Ø±ÙˆØ§ÙŠØ©ØŒ Ø¥Ù„Ø®)."""
    if not text: return ""
    # Ù‚Ø§Ø¦Ù…Ø© Ø£Ø·ÙˆÙ„ ÙˆØ£ÙƒØ«Ø± Ø¯Ù‚Ø© Ù„Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© Ø§Ù„ØªÙŠ Ù„Ø§ ØªØ¶ÙŠÙ Ù‚ÙŠÙ…Ø© Ù„Ù„Ø¨Ø­Ø«
    for word in ["ÙƒØªØ§Ø¨", "Ø±ÙˆØ§ÙŠØ©", "Ù†Ø³Ø®Ø©", "Ù…Ø¬Ù…ÙˆØ¹Ø©", "Ù…Ø¬Ù„Ø¯", "Ø¬Ø²Ø¡", "Ø·Ø¨Ø¹Ø©", "Ù…Ø¬Ø§Ù†ÙŠ", "ÙƒØ¨ÙŠØ±", "ØµØºÙŠØ±"]:
        text = text.replace(word, "")
    return text.strip()

def extract_keywords(text: str) -> List[str]:
    """ÙŠØ³ØªØ®Ù„Øµ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©ØŒ ÙˆÙŠÙØµÙ„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù‚ØµÙŠØ±Ø© Ø§Ù„Ù‡Ø§Ù…Ø©."""
    if not text:
        return []
    clean_text = normalize_text(text)
    words = clean_text.split()
    
    # Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© (Ù„Ø§ ØªØ´Ù…Ù„ Stop Words)
    keywords = [w for w in words if w not in ARABIC_STOP_WORDS and len(w) >= 1]
    
    # Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù‚ØµÙŠØ±Ø© (Ø¨Ù…Ø§ ÙÙŠ Ø°Ù„Ùƒ Stop Words) Ø§Ù„ØªÙŠ Ù‚Ø¯ ØªÙƒÙˆÙ† Ù…Ù‡Ù…Ø© Ù„Ù„Ø¨Ø­Ø« Ø§Ù„Ø¯Ù‚ÙŠÙ‚ ÙƒØ¹Ø¨Ø§Ø±Ø© ÙƒØ§Ù…Ù„Ø©
    stop_words_for_search = [w for w in words if w in ARABIC_STOP_WORDS]
    
    # Ù†Ø¯Ù…Ø¬ Ø§Ù„Ø¬Ù…ÙŠØ¹: Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© + Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù‚ØµÙŠØ±Ø©/Stop Words (Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙÙŠ calculate_score)
    return list(set(keywords + stop_words_for_search))

def get_db_safe_query(normalized_query: str) -> str:
    return normalized_query.replace("'", "''")

# -----------------------------
# ØªÙ‚Ø´ÙŠØ± Ø¨Ø³ÙŠØ· Ù„Ù„ÙƒÙ„Ù…Ø§Øª (light stemming)
# -----------------------------
def light_stem(word: str) -> str:
    """ØªÙ‚Ø´ÙŠØ± Ø®ÙÙŠÙ Ù„Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù„ÙˆØ§Ø­Ù‚ Ùˆ'Ø§Ù„Ù€' Ø§Ù„ØªØ¹Ø±ÙŠÙ."""
    suffixes = ["ÙŠØ©", "ÙŠ", "ÙˆÙ†", "Ø§Øª", "Ø§Ù†", "ÙŠÙ†", "Ù‡"]
    for suf in suffixes:
        if word.endswith(suf) and len(word) > len(suf) + 2:
            word = word[:-len(suf)]
            break
    if word.startswith("Ø§Ù„") and len(word) > 3:
        word = word[2:]
    
    # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø¹Ø¯Ù… ØªØ±Ùƒ Ø§Ù„ÙƒÙ„Ù…Ø© ÙØ§Ø±ØºØ© Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ‚Ø´ÙŠØ±
    return word if word else ""

# -----------------------------
# Ø¯ÙˆØ§Ù„ Ø§Ù„Ù…Ø±Ø§Ø¯ÙØ§Øª Ø§Ù„Ø¨Ø³ÙŠØ·Ø©
# -----------------------------
SYNONYMS = {
    "Ù…Ù‡Ù†Ø¯Ø³": ["Ù‡Ù†Ø¯Ø³Ø©", "Ù…Ù‚Ø§ÙˆÙ„", "Ù…Ø¹Ù…Ø§Ø±ÙŠ"],
    "Ø§Ù„Ù‡Ù†Ø¯Ø³Ø©": ["Ù…Ù‡Ù†Ø¯Ø³", "Ù…Ø¹Ù…Ø§Ø±", "Ø¨Ù†Ø§Ø¡"],
    "Ø§Ù„Ù…Ù‡Ø¯ÙŠ": ["Ø§Ù„Ù…Ù†Ù‚Ø°", "Ø§Ù„Ù‚Ø§Ø¦Ù…"],
    "Ø¹Ø¯Ù…ÙŠØ©": ["Ù†ÙŠØªØ´Ù‡", "Ù…ÙˆØª", "Ø¹Ø¨Ø«"], # Ù…Ø«Ø§Ù„ Ù„Ø±Ø¨Ø· Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù‚ØµÙŠØ±Ø© Ø¨Ù…ÙˆØ§Ø¶ÙŠØ¹Ù‡Ø§
    "Ø¯ÙŠÙ†": ["Ø¥Ø³Ù„Ø§Ù…", "Ù…Ø³ÙŠØ­ÙŠØ©", "ÙŠÙ‡ÙˆØ¯ÙŠØ©", "ÙÙ‚Ù‡"],
    "ÙÙ„Ø³ÙØ©": ["Ù…Ù†Ø·Ù‚", "Ù…ÙÙ‡ÙˆÙ…", "Ù…ØªØ§ÙÙŠØ²ÙŠÙ‚Ø§"],
}

def expand_keywords_with_synonyms(keywords: List[str]) -> List[str]:
    """ÙŠÙˆØ³Ø¹ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø¨Ù…Ø±Ø§Ø¯ÙØ§ØªÙ‡Ø§."""
    expanded = set(keywords)
    for k in keywords:
        if k in SYNONYMS:
            expanded.update(SYNONYMS[k])
    return list(expanded)

# -----------------------------
# Ø¯Ø§Ù„Ø© Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„ÙˆØ²Ù†ÙŠ Ø§Ù„Ù…ÙØ¹Ø²Ø²Ø© (Ù„Ø¶Ù…Ø§Ù† Ø¸Ù‡ÙˆØ± Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù‚ØµÙŠØ±Ø©)
# -----------------------------
def calculate_score(book: Dict[str, Any], query_keywords: List[str], normalized_query: str) -> int:
    """ØªØ­Ø³Ø¨ Ù†Ù‚Ø§Ø· Ø§Ù„ÙƒØªØ§Ø¨ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ø¬Ø°ÙˆØ± ÙˆØ§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù‚ØµÙŠØ±Ø© Ø§Ù„Ù‡Ø§Ù…Ø©."""
    score = 0
    book_name = normalize_text(book.get('file_name', ''))

    # 1. ğŸ¥‡ Ø£Ø¹Ù„Ù‰ ÙˆØ²Ù†: Ø§Ù„ØªØ·Ø§Ø¨Ù‚ Ø§Ù„ÙƒØ§Ù…Ù„ / Ø´Ø¨Ù‡ Ø§Ù„ÙƒØ§Ù…Ù„ (Exact Match)
    if normalized_query == book_name:
        score += 200 # Ø·ÙØ±Ø© Ù†ÙˆØ¹ÙŠØ© Ù„Ù„ØªØ·Ø§Ø¨Ù‚ Ø§Ù„ØªØ§Ù…
    elif normalized_query in book_name:
        score += 100

    title_words = book_name.split()
    
    for k in query_keywords:
        k_len = len(k)
        is_significant_short = k_len <= 3 and k not in ARABIC_STOP_WORDS # ÙƒÙ„Ù…Ø© Ù‚ØµÙŠØ±Ø© Ø°Ø§Øª Ø¯Ù„Ø§Ù„Ø© (Ù…Ø«Ù„: Ø¯ÙŠÙ†ØŒ ÙÙ†)

        # 2. âš–ï¸ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        base_match_score = 30 if k_len > 3 else 20 
        base_stem_score = 15 if k_len > 3 else 10

        k_stem = light_stem(k)
        if not k_stem: continue # ØªØ®Ø·ÙŠ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ØªÙŠ Ø§Ø®ØªÙØª Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ‚Ø´ÙŠØ±

        for t_word in title_words:
            t_stem = light_stem(t_word)
            
            # 3. â­ï¸ ÙˆØ²Ù† Ø§Ù„ØªØ·Ø§Ø¨Ù‚ ÙÙŠ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© (Ø§Ù„Ø£Ù‡Ù…)
            if t_stem.startswith(k_stem) and len(k_stem) >= 2:
                score += base_match_score * 2 # ÙˆØ²Ù† Ù…Ø¶Ø§Ø¹Ù
            
            # 4. ğŸ’« ÙˆØ²Ù† ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ø¬Ø°Ø± (Stem Match)
            elif k_stem in t_stem:
                score += base_stem_score
                
            # 5. âœ¨ ÙˆØ²Ù† ØªØ·Ø§Ø¨Ù‚ Ø§Ù„ÙƒÙ„Ù…Ø© Ø¨Ø§Ù„ÙƒØ§Ù…Ù„ (Partial Word Match)
            elif k in t_word:
                score += 5 
                
            # 6. ğŸ’¡ ØªØ¹Ø²ÙŠØ² Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù‚ØµÙŠØ±Ø© Ø§Ù„Ù‡Ø§Ù…Ø©
            if is_significant_short and k == t_word:
                score += 50 # ØªØ¹Ø²ÙŠØ² Ø¥Ø¶Ø§ÙÙŠ Ù„Ø¸Ù‡ÙˆØ± Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ù‚ØµÙŠØ±Ø© Ø§Ù„Ù‡Ø§Ù…Ø© ØªÙ…Ø§Ù…Ø§Ù‹
                
    return score

# -----------------------------
# Ø¥Ø´Ø¹Ø§Ø± Ø§Ù„Ù…Ø´Ø±Ù Ø¨Ø¹Ø¯ ÙƒÙ„ Ø¨Ø­Ø«
# -----------------------------
async def notify_admin_search(context: ContextTypes.DEFAULT_TYPE, username: str, query: str, found: bool):
    if ADMIN_USER_ID == 0: return
    bot = context.bot
    status_text = "âœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬" if found else "âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬"
    username_text = f"@{username}" if username else "(Ø¨Ø¯ÙˆÙ† ÙŠÙˆØ²Ø±)"
    message = f"ğŸ”” Ù‚Ø§Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… {username_text} Ø¨Ø§Ù„Ø¨Ø­Ø« Ø¹Ù†:\n`{query}`\nØ§Ù„Ø­Ø§Ù„Ø©: {status_text}"
    try:
        await bot.send_message(ADMIN_USER_ID, message, parse_mode='Markdown')
    except Exception as e:
        print(f"Failed to notify admin: {e}")

# -----------------------------
# Ø¥Ø±Ø³Ø§Ù„ ØµÙØ­Ø© Ø§Ù„ÙƒØªØ¨ 
# -----------------------------
async def send_books_page(update, context: ContextTypes.DEFAULT_TYPE, include_index_home: bool = False):
    books = context.user_data.get("search_results", [])
    page = context.user_data.get("current_page", 0)
    search_stage = context.user_data.get("search_stage", "ØªØ·Ø§Ø¨Ù‚ Ø¯Ù‚ÙŠÙ‚")
    total_pages = (len(books) - 1) // BOOKS_PER_PAGE + 1 if books else 1

    start = page * BOOKS_PER_PAGE
    end = start + BOOKS_PER_PAGE
    current_books = books[start:end]

    if "Ø¨Ø­Ø« Ù…ÙˆØ³Ø¹" in search_stage or "Ø§Ù„Ø¬Ø°ÙˆØ±" in search_stage:
        stage_note = "âš ï¸ Ù†ØªØ§Ø¦Ø¬ Ø¨Ø­Ø« Ù…ÙˆØ³Ø¹ (Ø¨Ø­Ø«Ù†Ø§ Ø¨Ø§Ù„Ø¬Ø°ÙˆØ± ÙˆØ§Ù„Ù…Ø±Ø§Ø¯ÙØ§Øª)"
    elif "ØªØ·Ø§Ø¨Ù‚ Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª" in search_stage:
        stage_note = "âœ… Ù†ØªØ§Ø¦Ø¬ Ø¯Ù„Ø§Ù„ÙŠØ© (ØªØ·Ø§Ø¨Ù‚ Ø¬Ù…ÙŠØ¹ ÙƒÙ„Ù…Ø§ØªÙƒ Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©)"
    else:
        stage_note = "âœ… Ù†ØªØ§Ø¦Ø¬ Ù…Ø·Ø§Ø¨Ù‚Ø© (ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ø¹Ø¨Ø§Ø±Ø© ÙƒØ§Ù…Ù„Ø©)"

    text = f"ğŸ“š Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ({len(books)} ÙƒØªØ§Ø¨)\n{stage_note}\nØ§Ù„ØµÙØ­Ø© {page + 1} Ù…Ù† {total_pages}\n\n"
    keyboard = []

    for b in current_books:
        if not b.get("file_name") or not b.get("file_id"):
            continue
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… hash Ù„Ù€ file_id Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙØªØ§Ø­ Ø¢Ù…Ù† Ù„Ù„ÙƒÙˆÙ„Ø¨Ø§Ùƒ
        key = hashlib.md5(b["file_id"].encode()).hexdigest()[:16]
        context.bot_data[f"file_{key}"] = b["file_id"]
        keyboard.append([InlineKeyboardButton(f"ğŸ“˜ {b['file_name']}", callback_data=f"file:{key}")])

    nav_buttons = []
    if page > 0:
        nav_buttons.append(InlineKeyboardButton("â¬…ï¸ Ø§Ù„Ø³Ø§Ø¨Ù‚", callback_data="prev_page"))
    if end < len(books):
        nav_buttons.append(InlineKeyboardButton("Ø§Ù„ØªØ§Ù„ÙŠ â¡ï¸", callback_data="next_page"))
    if nav_buttons:
        keyboard.append(nav_buttons)

    # Ø²Ø± Ø§Ù„Ø¹ÙˆØ¯Ø© Ù„Ù„ÙÙ‡Ø±Ø³ ÙŠØ¸Ù‡Ø± Ø¯Ø§Ø¦Ù…Ù‹Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„ÙƒØªØ¨ Ø¶Ù…Ù† ÙÙ‡Ø±Ø³
    if context.user_data.get("is_index", False) or include_index_home:
        keyboard.append([InlineKeyboardButton("ğŸ  Ø§Ù„Ø¹ÙˆØ¯Ø© Ù„Ù„ÙÙ‡Ø±Ø³", callback_data="home_index")])

    reply_markup = InlineKeyboardMarkup(keyboard)
    if update.message:
        await update.message.reply_text(text, reply_markup=reply_markup)
    elif update.callback_query:
        await update.callback_query.message.edit_text(text, reply_markup=reply_markup)

# -----------------------------
# Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø°ÙƒÙŠ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ù…Ø±Ø§Ø­Ù„ (Ø·ÙØ±Ø© Ù†ÙˆØ¹ÙŠØ©)
# -----------------------------
async def search_books(update, context: ContextTypes.DEFAULT_TYPE):
    if update.effective_chat.type != "private": return
    query = update.message.text.strip()
    if not query: return

    conn = context.bot_data.get("db_conn")
    if not conn:
        await update.message.reply_text("âŒ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± Ù…ØªØµÙ„Ø© Ø­Ø§Ù„ÙŠØ§Ù‹.")
        return

    # ğŸ’¡ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³Ø¨Ù‚Ø© Ù„Ù„Ø¨Ø­Ø«
    normalized_query = normalize_text(remove_common_words(query))
    all_words_in_query = normalize_text(query).split() # ÙƒÙ„ ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… (Ø¨Ù…Ø§ ÙÙŠ Ø°Ù„Ùƒ Stop Words)
    
    # Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© (Ø¨Ø¯ÙˆÙ† Stop Words)
    keywords = [w for w in all_words_in_query if w not in ARABIC_STOP_WORDS and len(w) >= 1] 
    
    # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¬Ø°ÙˆØ± ÙˆØ§Ù„Ù…Ø±Ø§Ø¯ÙØ§Øª Ù„Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ÙˆØ³Ø¹
    expanded_keywords = expand_keywords_with_synonyms(keywords)
    stemmed_keywords = [light_stem(k) for k in expanded_keywords]
    
    context.user_data["last_query"] = normalized_query
    context.user_data["last_keywords"] = keywords

    books = []
    search_stage_text = "ØªØ·Ø§Ø¨Ù‚ Ø¯Ù‚ÙŠÙ‚ (Ø§Ù„Ø¹Ø¨Ø§Ø±Ø© ÙƒØ§Ù…Ù„Ø©)"
    
    try:
        # 1. ğŸ¥‡ Ø§Ù„Ù…Ø±Ø­Ù„Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰: ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ø¹Ø¨Ø§Ø±Ø© ÙƒØ§Ù…Ù„Ø©
        books = await conn.fetch("""
            SELECT id, file_id, file_name, uploaded_at
            FROM books
            WHERE LOWER(file_name) LIKE '%' || $1 || '%'
            ORDER BY uploaded_at DESC;
        """, normalized_query)
        
        # 2. ğŸ¥ˆ Ø§Ù„Ù…Ø±Ø­Ù„Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ©: ØªØ·Ø§Ø¨Ù‚ Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© (Ø§Ù„Ù…Ù‚Ø´Ø±Ø© Ø£Ùˆ ØºÙŠØ± Ø§Ù„Ù…Ù‚Ø´Ø±Ø©)
        if not books and keywords:
            search_stage_text = "ØªØ·Ø§Ø¨Ù‚ Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©"
            # Ø´Ø±Ø· AND ÙŠØ¬Ù…Ø¹ ØªØ·Ø§Ø¨Ù‚ Ø§Ù„ÙƒÙ„Ù…Ø© Ø£Ùˆ Ø¬Ø°Ø±Ù‡Ø§ Ù„Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø¯Ù‚Ø©
            all_match_conditions = " AND ".join([
                f"(LOWER(file_name) LIKE '%{get_db_safe_query(k)}%' OR LOWER(file_name) LIKE '%{get_db_safe_query(light_stem(k))}%')"
                for k in keywords if len(k) >= 2
            ])
            if all_match_conditions:
                 books = await conn.fetch(f"""
                    SELECT id, file_id, file_name, uploaded_at
                    FROM books
                    WHERE {all_match_conditions}
                    ORDER BY uploaded_at DESC;
                """)

        # 3. ğŸ¥‰ Ø§Ù„Ù…Ø±Ø­Ù„Ø© Ø§Ù„Ø«Ø§Ù„Ø«Ø©: Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ÙˆØ³Ø¹ Ø¨Ø§Ù„Ø¬Ø°ÙˆØ± ÙˆØ§Ù„Ù…Ø±Ø§Ø¯ÙØ§Øª (OR Conditions)
        if not books and expanded_keywords:
            search_stage_text = "Ø¨Ø­Ø« Ù…ÙˆØ³Ø¹ Ø¨Ø§Ù„Ø¬Ø°ÙˆØ± ÙˆØ§Ù„Ù…Ø±Ø§Ø¯ÙØ§Øª"
            
            search_terms = list(set(expanded_keywords + stemmed_keywords))
            or_conditions = " OR ".join([
                f"LOWER(file_name) LIKE '%{get_db_safe_query(k)}%'"
                for k in search_terms if len(k) >= 2
            ])
            
            if or_conditions:
                books = await conn.fetch(f"""
                    SELECT id, file_id, file_name, uploaded_at
                    FROM books
                    WHERE {or_conditions}
                    ORDER BY uploaded_at DESC;
                """)
        
        # 4. ğŸ’¡ Ø§Ù„Ù…Ø±Ø­Ù„Ø© Ø§Ù„Ø±Ø§Ø¨Ø¹Ø©: Ø¶Ù…Ø§Ù† Ø¸Ù‡ÙˆØ± Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù‚ØµÙŠØ±Ø© Ø§Ù„Ù‡Ø§Ù…Ø© (Ø§Ù„Ø¨Ø­Ø« Ø¨Ø§Ù„Ø¹Ø¨Ø§Ø±Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ©)
        if not books and len(normalized_query.split()) == 1 and normalized_query not in ARABIC_STOP_WORDS:
            search_stage_text = "Ø¨Ø­Ø« Ø®Ø§Øµ Ø¨Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù‚ØµÙŠØ±Ø© Ø§Ù„Ù‡Ø§Ù…Ø©"
            books = await conn.fetch("""
                SELECT id, file_id, file_name, uploaded_at
                FROM books
                WHERE LOWER(file_name) LIKE '%' || $1 || '%'
                ORDER BY uploaded_at DESC;
            """, normalized_query)


    except Exception as e:
        await update.message.reply_text(f"âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø«: {e}")
        return

    found_results = bool(books)
    await notify_admin_search(context, update.effective_user.username, query, found_results)

    if not books:
        keyboard = InlineKeyboardMarkup([[InlineKeyboardButton("ğŸ” Ø¨Ø­Ø« Ø¹Ù† ÙƒØªØ¨ Ù…Ø´Ø§Ø¨Ù‡Ø©", callback_data="search_similar")]])
        await update.message.reply_text(f"âŒ Ù„Ù… Ø£Ø¬Ø¯ Ø£ÙŠ ÙƒØªØ¨ Ù…Ø·Ø§Ø¨Ù‚Ø© Ù„Ù„Ø¨Ø­Ø«: {query}\nÙŠÙ…ÙƒÙ†Ùƒ ØªØ¬Ø±Ø¨Ø© Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙƒØªØ¨ Ù…Ø´Ø§Ø¨Ù‡Ø©:", reply_markup=keyboard)
        context.user_data["search_results"] = []
        context.user_data["current_page"] = 0
        return

    # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„ÙˆØ²Ù†ÙŠ Ø§Ù„Ù…ÙØ¹Ø²Ø² Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    scored_books = []
    for book in books:
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… all_words_in_query ÙƒÙ€ query_keywords ÙÙŠ calculate_score Ù„Ø¶Ù…Ø§Ù† Ø§Ø­ØªØ³Ø§Ø¨ Ù†Ù‚Ø§Ø· Ù„Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù‚ØµÙŠØ±Ø©
        score = calculate_score(book, all_words_in_query, normalized_query) 
        book_dict = dict(book)
        book_dict['score'] = score
        scored_books.append(book_dict)

    # Ø§Ù„ÙØ±Ø² Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰: 1. Ø§Ù„Ù†Ù‚Ø§Ø· (Ø§Ù„Ø£Ø¹Ù„Ù‰) 2. ØªØ§Ø±ÙŠØ® Ø§Ù„Ø±ÙØ¹ (Ø§Ù„Ø£Ø­Ø¯Ø«)
    scored_books.sort(key=lambda b: (b['score'], b['uploaded_at']), reverse=True)
    
    context.user_data["search_results"] = scored_books
    context.user_data["current_page"] = 0
    context.user_data["search_stage"] = search_stage_text
    await send_books_page(update, context)

# -----------------------------
# Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙƒØªØ¨ Ù…Ø´Ø§Ø¨Ù‡Ø©
# -----------------------------
async def search_similar_books(update, context: ContextTypes.DEFAULT_TYPE):
    conn = context.bot_data.get("db_conn")
    keywords = context.user_data.get("last_keywords")
    last_query = context.user_data.get("last_query", "")
    
    if not keywords or not conn:
        await update.callback_query.message.reply_text("âŒ Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù…ÙˆØ¶ÙˆØ¹ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù†Ù‡.")
        return

    try:
        # Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ÙˆØ³Ø¹ Ø¨Ø§Ù„Ø¬Ø°ÙˆØ± ÙˆØ§Ù„Ù…Ø±Ø§Ø¯ÙØ§Øª
        expanded_keywords = expand_keywords_with_synonyms(keywords)
        stemmed_keywords = [light_stem(k) for k in expanded_keywords]
        
        search_terms = list(set(expanded_keywords + stemmed_keywords))
        or_conditions = " OR ".join([
            f"LOWER(file_name) LIKE '%{get_db_safe_query(k)}%'"
            for k in search_terms if len(k) >= 2
        ])
        
        if not or_conditions:
            await update.callback_query.message.reply_text("âŒ Ù„Ù… Ø£Ø¬Ø¯ ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© ØµØ§Ù„Ø­Ø© Ù„Ù„Ø¨Ø­Ø« Ø§Ù„Ù…Ø´Ø§Ø¨Ù‡.")
            return
            
        books = await conn.fetch(f"""
            SELECT id, file_id, file_name, uploaded_at
            FROM books
            WHERE {or_conditions}
            ORDER BY uploaded_at DESC;
        """)
        
    except Exception as e:
        await update.callback_query.message.edit_text(f"âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙƒØªØ¨ Ù…Ø´Ø§Ø¨Ù‡Ø©: {e}")
        return

    scored_books = []
    for book in books:
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ù„Ù„Ø§Ø­ØªØ³Ø§Ø¨
        all_words_in_query = normalize_text(last_query).split() 
        score = calculate_score(book, all_words_in_query, last_query)
        book_dict = dict(book)
        book_dict['score'] = score
        scored_books.append(book_dict)

    scored_books.sort(key=lambda b: (b['score'], b['uploaded_at']), reverse=True)

    if not scored_books:
        await update.callback_query.message.edit_text("âŒ Ù„Ù… Ø£Ø¬Ø¯ ÙƒØªØ¨ Ù…Ø´Ø§Ø¨Ù‡Ø©.")
        return

    context.user_data["search_results"] = scored_books
    context.user_data["current_page"] = 0
    context.user_data["search_stage"] = "Ø¨Ø­Ø« Ù…ÙˆØ³Ø¹ (Ù…Ø´Ø§Ø¨Ù‡ Ø¨Ø§Ù„Ø¬Ø°ÙˆØ± ÙˆØ§Ù„Ù…Ø±Ø§Ø¯ÙØ§Øª)"
    await send_books_page(update, context)

# -----------------------------
# Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø£Ø²Ø±Ø§Ø± Ø§Ù„ÙƒØªØ¨ + Ø£Ø²Ø±Ø§Ø± Ø§Ù„ÙÙ‡Ø±Ø³
# -----------------------------
async def handle_callbacks(update, context: ContextTypes.DEFAULT_TYPE):
    query = update.callback_query
    await query.answer()
    data = query.data

    if data.startswith("file:"):
        key = data.split(":")[1]
        file_id = context.bot_data.get(f"file_{key}")
        if file_id:
            caption = "ØªÙ… Ø§Ù„ØªÙ†Ø²ÙŠÙ„ Ø¨ÙˆØ§Ø³Ø·Ø© @boooksfree1bot"
            share_button = InlineKeyboardMarkup([
                [InlineKeyboardButton("ğŸ“¤ Ø´Ø§Ø±Ùƒ Ø§Ù„Ø¨ÙˆØª Ù…Ø¹ Ø£ØµØ¯Ù‚Ø§Ø¦Ùƒ", switch_inline_query="")]
            ])
            await query.message.reply_document(document=file_id, caption=caption, reply_markup=share_button)
        else:
            await query.message.reply_text("âŒ Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…ØªÙˆÙØ± Ø­Ø§Ù„ÙŠØ§Ù‹.")

    elif data == "next_page":
        context.user_data["current_page"] += 1
        await send_books_page(update, context)
    elif data == "prev_page":
        context.user_data["current_page"] -= 1
        await send_books_page(update, context)
    elif data == "search_similar":
        await search_similar_books(update, context)

    elif data == "home_index" or data == "show_index":
        # ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ù„Ø¯ÙŠÙƒ Ù…Ù„Ù index_handler.py
        from index_handler import show_index
        await show_index(update, context)
    elif data.startswith("index_page:"):
        from index_handler import navigate_index_pages
        await navigate_index_pages(update, context)
    elif data.startswith("index:"):
        from index_handler import search_by_index
        await search_by_index(update, context)
